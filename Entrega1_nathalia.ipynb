{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "01S9c2baxO7g",
        "AmsDwmbucfYI",
        "tgZeEgiafR5z",
        "YQzKUgjxe3KN",
        "RWoMcYKVe5Sc",
        "5HUl2TBhfec7"
      ],
      "authorship_tag": "ABX9TyM76328wAccdooehfLTxUdw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathlia/Aulas2023/blob/RECUPERA%C3%87%C3%83O-DE-INFORMA%C3%87%C3%83O-E-PERSONALIZA%C3%87%C3%83O-NA-WEB-(DPADP0152)/Entrega1_nathalia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import tfidVectorizer\n"
      ],
      "metadata": {
        "id": "bZRCSsSOz_tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports e iniciando as files"
      ],
      "metadata": {
        "id": "723p12h_-veE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "y8AN6cB33OU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Use the files.upload() method to upload files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "LCzhYt973Qnk",
        "outputId": "f930ac4d-43cd-4280-8f4f-baa3d51c1a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5aacadb2-fcea-49d6-84e8-19b9461300e9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5aacadb2-fcea-49d6-84e8-19b9461300e9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dezipar(n usei)"
      ],
      "metadata": {
        "id": "ifNEaSSz0E-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the path to the zip file\n",
        "zip_file_path = '/content/drive/MyDrive/tech.zip'\n",
        "\n",
        "# Specify the directory where you want to extract the contents\n",
        "extract_dir = '/content/drive/MyDrive/tech'\n",
        "\n",
        "# Open the zip file and extract its contents\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n"
      ],
      "metadata": {
        "id": "2xEh46Y3AmcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importando os stopwords"
      ],
      "metadata": {
        "id": "OeXBYBnm0Kks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "rz1c5jo7Pdcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86949fdc-48f2-4bdb-c7d0-defa185cd60e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "## Library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import os, glob, re, sys, random, unicodedata, collections\n",
        "from tqdm import tqdm\n",
        "from functools import reduce\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "from nltk.tokenize import sent_tokenize , word_tokenize\n",
        "\n",
        "STOP_WORDS = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(STOP_WORDS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO9rP_Zu5dVG",
        "outputId": "a9871af5-9c88-4cc3-a69a-f8ce8b438663"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'once', 'off', 'when', 'and', 'with', 'are', 'into', 'below', 'herself', 'after', 'then', 'doing', \"won't\", \"you'll\", 'had', \"she's\", 'a', 'does', 'his', 'isn', 'ours', 'there', \"hadn't\", 'up', 'each', 'ma', 'until', 'what', 'being', 'not', 'same', 'over', 'hadn', 'before', 'whom', 's', \"shan't\", 'it', \"needn't\", 'am', 'at', \"haven't\", 'my', 'as', 'hers', 'their', 'should', 'needn', 'won', 'o', \"you've\", 'your', \"doesn't\", 'her', \"you'd\", 'about', 'has', 'how', 'themselves', \"couldn't\", 'both', 'other', 'some', \"isn't\", 'by', \"wouldn't\", 'if', 'these', 'on', 'doesn', \"mustn't\", 'did', 'any', 'more', 'wouldn', 'didn', 'myself', 'they', 'during', 'such', 'while', \"aren't\", 'because', 'can', 'of', 'couldn', 'yourself', 'but', 'wasn', \"it's\", \"wasn't\", 'where', 'll', 're', 'out', 'shan', 'weren', \"hasn't\", 'you', 't', 'further', 'have', 'been', 'from', 'aren', 'that', 'theirs', \"that'll\", 'which', 'ourselves', 'again', \"didn't\", 'me', 'were', 'for', 'to', 've', \"shouldn't\", 'i', 'than', 'him', 'we', 'or', 'yourselves', 'itself', 'down', 'them', 'under', 'mightn', 'an', \"weren't\", 'himself', 'so', 'most', 'why', 'will', 'do', \"don't\", 'was', 'having', 'its', 'very', 'mustn', 'd', 'be', 'all', 'those', 'who', 'no', 'now', 'ain', 'haven', 'our', 'don', 'she', 'nor', 'm', 'above', 'is', 'between', 'yours', 'in', 'through', 'against', \"should've\", 'few', 'only', \"you're\", \"mightn't\", 'y', 'he', 'shouldn', 'too', 'this', 'own', 'just', 'here', 'hasn', 'the'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_list = list(STOP_WORDS)\n",
        "print(stop_words_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NcMrFN1LoAN",
        "outputId": "c99dae69-0bc0-4733-db1f-f3e7bae212d8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['once', 'off', 'when', 'and', 'with', 'are', 'into', 'below', 'herself', 'after', 'then', 'doing', \"won't\", \"you'll\", 'had', \"she's\", 'a', 'does', 'his', 'isn', 'ours', 'there', \"hadn't\", 'up', 'each', 'ma', 'until', 'what', 'being', 'not', 'same', 'over', 'hadn', 'before', 'whom', 's', \"shan't\", 'it', \"needn't\", 'am', 'at', \"haven't\", 'my', 'as', 'hers', 'their', 'should', 'needn', 'won', 'o', \"you've\", 'your', \"doesn't\", 'her', \"you'd\", 'about', 'has', 'how', 'themselves', \"couldn't\", 'both', 'other', 'some', \"isn't\", 'by', \"wouldn't\", 'if', 'these', 'on', 'doesn', \"mustn't\", 'did', 'any', 'more', 'wouldn', 'didn', 'myself', 'they', 'during', 'such', 'while', \"aren't\", 'because', 'can', 'of', 'couldn', 'yourself', 'but', 'wasn', \"it's\", \"wasn't\", 'where', 'll', 're', 'out', 'shan', 'weren', \"hasn't\", 'you', 't', 'further', 'have', 'been', 'from', 'aren', 'that', 'theirs', \"that'll\", 'which', 'ourselves', 'again', \"didn't\", 'me', 'were', 'for', 'to', 've', \"shouldn't\", 'i', 'than', 'him', 'we', 'or', 'yourselves', 'itself', 'down', 'them', 'under', 'mightn', 'an', \"weren't\", 'himself', 'so', 'most', 'why', 'will', 'do', \"don't\", 'was', 'having', 'its', 'very', 'mustn', 'd', 'be', 'all', 'those', 'who', 'no', 'now', 'ain', 'haven', 'our', 'don', 'she', 'nor', 'm', 'above', 'is', 'between', 'yours', 'in', 'through', 'against', \"should've\", 'few', 'only', \"you're\", \"mightn't\", 'y', 'he', 'shouldn', 'too', 'this', 'own', 'just', 'here', 'hasn', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mounted google drive para acessar os textos, escolhi a pasta tech e importei para o drive"
      ],
      "metadata": {
        "id": "vmP6lNbB0Npl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRzYgZKes_gq",
        "outputId": "23c323a8-eb27-4a20-ad91-3c7f33a2e3bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to your Google Drive directory\n",
        "google_drive_path = '/content/drive/MyDrive/MyDrive/ENTREGA1/tech'\n",
        "\n",
        "# List the files and subdirectories in your Google Drive directory\n",
        "file_list = os.listdir(google_drive_path)\n",
        "\n",
        "# Count the number of files\n",
        "file_count = len(file_list)\n",
        "\n",
        "# Print the count\n",
        "print(f\"Number of files in your Google Drive directory: {file_count}\")"
      ],
      "metadata": {
        "id": "zBPnQ2JrpzEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd7f8d3-45e2-42bb-a9a9-c1164cd33612"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in your Google Drive directory: 401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n"
      ],
      "metadata": {
        "id": "931goM_P8QvB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
        "files = os.listdir(google_drive_path)  # Get all the files in that directory\n",
        "print(\"Files in %r: %s\" % (cwd, files))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT0K4QYs8gSi",
        "outputId": "d263a0b0-35c3-4b86-edeb-68921b82243c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in '/content': ['023.txt', '020.txt', '018.txt', '016.txt', '021.txt', '015.txt', '017.txt', '019.txt', '022.txt', '014.txt', '012.txt', '010.txt', '008.txt', '013.txt', '009.txt', '005.txt', '007.txt', '011.txt', '006.txt', '024.txt', '054.txt', '053.txt', '050.txt', '045.txt', '049.txt', '047.txt', '048.txt', '046.txt', '051.txt', '052.txt', '044.txt', '033.txt', '026.txt', '031.txt', '028.txt', '029.txt', '025.txt', '027.txt', '043.txt', '037.txt', '041.txt', '040.txt', '035.txt', '036.txt', '038.txt', '039.txt', '042.txt', '034.txt', '030.txt', '032.txt', '084.txt', '073.txt', '066.txt', '065.txt', '068.txt', '069.txt', '071.txt', '083.txt', '080.txt', '078.txt', '081.txt', '075.txt', '076.txt', '079.txt', '077.txt', '082.txt', '074.txt', '067.txt', '070.txt', '063.txt', '060.txt', '059.txt', '055.txt', '061.txt', '057.txt', '058.txt', '072.txt', '064.txt', '056.txt', '062.txt', '113.txt', '111.txt', '110.txt', '106.txt', '105.txt', '107.txt', '109.txt', '108.txt', '112.txt', '104.txt', '093.txt', '090.txt', '087.txt', '089.txt', '091.txt', '086.txt', '088.txt', '102.txt', '103.txt', '100.txt', '099.txt', '095.txt', '096.txt', '097.txt', '098.txt', '101.txt', '094.txt', '085.txt', '092.txt', '133.txt', '125.txt', '126.txt', '129.txt', '127.txt', '131.txt', '128.txt', '136.txt', '137.txt', '135.txt', '139.txt', '141.txt', '142.txt', '134.txt', '130.txt', '132.txt', '124.txt', '123.txt', '120.txt', '121.txt', '119.txt', '116.txt', '118.txt', '115.txt', '117.txt', '122.txt', '114.txt', '165.txt', '168.txt', '167.txt', '166.txt', '170.txt', '171.txt', '163.txt', '155.txt', '161.txt', '157.txt', '160.txt', '158.txt', '156.txt', '172.txt', '164.txt', '159.txt', '152.txt', '148.txt', '146.txt', '145.txt', '150.txt', '147.txt', '149.txt', '162.txt', '154.txt', '151.txt', '153.txt', '143.txt', '140.txt', '138.txt', '144.txt', '200.txt', '202.txt', '194.txt', '193.txt', '188.txt', '189.txt', '191.txt', '185.txt', '190.txt', '186.txt', '187.txt', '192.txt', '183.txt', '177.txt', '178.txt', '181.txt', '179.txt', '175.txt', '180.txt', '184.txt', '176.txt', '182.txt', '174.txt', '173.txt', '169.txt', '232.txt', '203.txt', '195.txt', '201.txt', '199.txt', '196.txt', '197.txt', '198.txt', '224.txt', '223.txt', '216.txt', '220.txt', '219.txt', '218.txt', '221.txt', '217.txt', '213.txt', '210.txt', '207.txt', '208.txt', '209.txt', '205.txt', '211.txt', '215.txt', '222.txt', '214.txt', '206.txt', '212.txt', '204.txt', '254.txt', '243.txt', '238.txt', '235.txt', '237.txt', '240.txt', '241.txt', '236.txt', '253.txt', '249.txt', '250.txt', '246.txt', '248.txt', '247.txt', '251.txt', '245.txt', '252.txt', '244.txt', '242.txt', '239.txt', '234.txt', '233.txt', '230.txt', '228.txt', '225.txt', '227.txt', '229.txt', '231.txt', '226.txt', '284.txt', '283.txt', '277.txt', '278.txt', '275.txt', '281.txt', '280.txt', '276.txt', '273.txt', '267.txt', '266.txt', '265.txt', '271.txt', '270.txt', '268.txt', '279.txt', '282.txt', '272.txt', '274.txt', '263.txt', '259.txt', '260.txt', '257.txt', '258.txt', '256.txt', '255.txt', '269.txt', '264.txt', '261.txt', '262.txt', '313.txt', '311.txt', '305.txt', '309.txt', '306.txt', '307.txt', '310.txt', '303.txt', '296.txt', '295.txt', '299.txt', '301.txt', '298.txt', '297.txt', '308.txt', '312.txt', '304.txt', '293.txt', '288.txt', '287.txt', '286.txt', '285.txt', '290.txt', '291.txt', '300.txt', '302.txt', '294.txt', '289.txt', '292.txt', '339.txt', '336.txt', '342.txt', '334.txt', '333.txt', '331.txt', '325.txt', '329.txt', '330.txt', '326.txt', '327.txt', '324.txt', '328.txt', '323.txt', '321.txt', '320.txt', '319.txt', '318.txt', '317.txt', '316.txt', '332.txt', '315.txt', '322.txt', '314.txt', '364.txt', '363.txt', '357.txt', '358.txt', '360.txt', '356.txt', '355.txt', '361.txt', '353.txt', '350.txt', '348.txt', '346.txt', '345.txt', '349.txt', '351.txt', '359.txt', '362.txt', '354.txt', '347.txt', '343.txt', '335.txt', '341.txt', '340.txt', '338.txt', '337.txt', '352.txt', '344.txt', '394.txt', '393.txt', '389.txt', '390.txt', '386.txt', '391.txt', '385.txt', '387.txt', '388.txt', '392.txt', '384.txt', '383.txt', '379.txt', '378.txt', '375.txt', '376.txt', '377.txt', '380.txt', '381.txt', '382.txt', '373.txt', '365.txt', '371.txt', '370.txt', '368.txt', '369.txt', '374.txt', '366.txt', '367.txt', '372.txt', '400.txt', '395.txt', '399.txt', '396.txt', '398.txt', '397.txt', '004.txt', '003.txt', '002.txt', '401.txt', '001.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select and read 10 random files\n",
        "sample_texts = files\n",
        "# sample_texts = random.sample(file_list, 100)\n",
        "print(\"Lista de textos:\\n\", sample_texts)\n",
        "\n",
        "docs = []\n",
        "for fname in sample_texts:\n",
        "    with open(google_drive_path + '/' + fname , \"r\", encoding=\"ISO-8859-1\") as file:\n",
        "        text = file.read()\n",
        "    docs.append(text)\n",
        "\n",
        "print(\"Finished reading files in:\", google_drive_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTqF3Rv82opb",
        "outputId": "1cc46704-e3a8-4902-b998-9ae819d84795"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lista de textos:\n",
            " ['023.txt', '020.txt', '018.txt', '016.txt', '021.txt', '015.txt', '017.txt', '019.txt', '022.txt', '014.txt', '012.txt', '010.txt', '008.txt', '013.txt', '009.txt', '005.txt', '007.txt', '011.txt', '006.txt', '024.txt', '054.txt', '053.txt', '050.txt', '045.txt', '049.txt', '047.txt', '048.txt', '046.txt', '051.txt', '052.txt', '044.txt', '033.txt', '026.txt', '031.txt', '028.txt', '029.txt', '025.txt', '027.txt', '043.txt', '037.txt', '041.txt', '040.txt', '035.txt', '036.txt', '038.txt', '039.txt', '042.txt', '034.txt', '030.txt', '032.txt', '084.txt', '073.txt', '066.txt', '065.txt', '068.txt', '069.txt', '071.txt', '083.txt', '080.txt', '078.txt', '081.txt', '075.txt', '076.txt', '079.txt', '077.txt', '082.txt', '074.txt', '067.txt', '070.txt', '063.txt', '060.txt', '059.txt', '055.txt', '061.txt', '057.txt', '058.txt', '072.txt', '064.txt', '056.txt', '062.txt', '113.txt', '111.txt', '110.txt', '106.txt', '105.txt', '107.txt', '109.txt', '108.txt', '112.txt', '104.txt', '093.txt', '090.txt', '087.txt', '089.txt', '091.txt', '086.txt', '088.txt', '102.txt', '103.txt', '100.txt', '099.txt', '095.txt', '096.txt', '097.txt', '098.txt', '101.txt', '094.txt', '085.txt', '092.txt', '133.txt', '125.txt', '126.txt', '129.txt', '127.txt', '131.txt', '128.txt', '136.txt', '137.txt', '135.txt', '139.txt', '141.txt', '142.txt', '134.txt', '130.txt', '132.txt', '124.txt', '123.txt', '120.txt', '121.txt', '119.txt', '116.txt', '118.txt', '115.txt', '117.txt', '122.txt', '114.txt', '165.txt', '168.txt', '167.txt', '166.txt', '170.txt', '171.txt', '163.txt', '155.txt', '161.txt', '157.txt', '160.txt', '158.txt', '156.txt', '172.txt', '164.txt', '159.txt', '152.txt', '148.txt', '146.txt', '145.txt', '150.txt', '147.txt', '149.txt', '162.txt', '154.txt', '151.txt', '153.txt', '143.txt', '140.txt', '138.txt', '144.txt', '200.txt', '202.txt', '194.txt', '193.txt', '188.txt', '189.txt', '191.txt', '185.txt', '190.txt', '186.txt', '187.txt', '192.txt', '183.txt', '177.txt', '178.txt', '181.txt', '179.txt', '175.txt', '180.txt', '184.txt', '176.txt', '182.txt', '174.txt', '173.txt', '169.txt', '232.txt', '203.txt', '195.txt', '201.txt', '199.txt', '196.txt', '197.txt', '198.txt', '224.txt', '223.txt', '216.txt', '220.txt', '219.txt', '218.txt', '221.txt', '217.txt', '213.txt', '210.txt', '207.txt', '208.txt', '209.txt', '205.txt', '211.txt', '215.txt', '222.txt', '214.txt', '206.txt', '212.txt', '204.txt', '254.txt', '243.txt', '238.txt', '235.txt', '237.txt', '240.txt', '241.txt', '236.txt', '253.txt', '249.txt', '250.txt', '246.txt', '248.txt', '247.txt', '251.txt', '245.txt', '252.txt', '244.txt', '242.txt', '239.txt', '234.txt', '233.txt', '230.txt', '228.txt', '225.txt', '227.txt', '229.txt', '231.txt', '226.txt', '284.txt', '283.txt', '277.txt', '278.txt', '275.txt', '281.txt', '280.txt', '276.txt', '273.txt', '267.txt', '266.txt', '265.txt', '271.txt', '270.txt', '268.txt', '279.txt', '282.txt', '272.txt', '274.txt', '263.txt', '259.txt', '260.txt', '257.txt', '258.txt', '256.txt', '255.txt', '269.txt', '264.txt', '261.txt', '262.txt', '313.txt', '311.txt', '305.txt', '309.txt', '306.txt', '307.txt', '310.txt', '303.txt', '296.txt', '295.txt', '299.txt', '301.txt', '298.txt', '297.txt', '308.txt', '312.txt', '304.txt', '293.txt', '288.txt', '287.txt', '286.txt', '285.txt', '290.txt', '291.txt', '300.txt', '302.txt', '294.txt', '289.txt', '292.txt', '339.txt', '336.txt', '342.txt', '334.txt', '333.txt', '331.txt', '325.txt', '329.txt', '330.txt', '326.txt', '327.txt', '324.txt', '328.txt', '323.txt', '321.txt', '320.txt', '319.txt', '318.txt', '317.txt', '316.txt', '332.txt', '315.txt', '322.txt', '314.txt', '364.txt', '363.txt', '357.txt', '358.txt', '360.txt', '356.txt', '355.txt', '361.txt', '353.txt', '350.txt', '348.txt', '346.txt', '345.txt', '349.txt', '351.txt', '359.txt', '362.txt', '354.txt', '347.txt', '343.txt', '335.txt', '341.txt', '340.txt', '338.txt', '337.txt', '352.txt', '344.txt', '394.txt', '393.txt', '389.txt', '390.txt', '386.txt', '391.txt', '385.txt', '387.txt', '388.txt', '392.txt', '384.txt', '383.txt', '379.txt', '378.txt', '375.txt', '376.txt', '377.txt', '380.txt', '381.txt', '382.txt', '373.txt', '365.txt', '371.txt', '370.txt', '368.txt', '369.txt', '374.txt', '366.txt', '367.txt', '372.txt', '400.txt', '395.txt', '399.txt', '396.txt', '398.txt', '397.txt', '004.txt', '003.txt', '002.txt', '401.txt', '001.txt']\n",
            "Finished reading files in: /content/drive/MyDrive/MyDrive/ENTREGA1/tech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count term frequency using CountVectorizer from scikit-learn\n",
        "## limiting number of words just for illustrating the concept\n",
        "\n",
        "vec = CountVectorizer(max_features=20, stop_words=stop_words_list)\n",
        "X = vec.fit_transform(docs)\n",
        "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())\n",
        "files_names = [file.split('/')[-1] for file in sample_texts]\n",
        "df['file'] = files_names\n",
        "df = df.set_index('file')\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR0REeZ0KEcl",
        "outputId": "8a761047-9802-4113-e8ca-977d86f1d2a0"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         also  could  digital  games  many  mobile  mr  music  net  new  one  \\\n",
            "file                                                                           \n",
            "023.txt     2      0        0      0     0       0   3      0    0    0    0   \n",
            "020.txt     2      0        0      0     1       0   1      0    0    6    1   \n",
            "018.txt     2      3        1      3     0       0   0      0    0    1    0   \n",
            "016.txt     0      2        0      0     1       0   5      0    0    1    6   \n",
            "021.txt     0      1        0      0     2       0   1      0    1    1    1   \n",
            "...       ...    ...      ...    ...   ...     ...  ..    ...  ...  ...  ...   \n",
            "004.txt     1      2        4      1     0       1   4      0    0    0    3   \n",
            "003.txt     1      0        0      0     0       0   0      0    0    0    0   \n",
            "002.txt     3      0        0      1     1       0   0      0   15    1    1   \n",
            "401.txt     0      5        0     22     4       0   0      0    0    2    7   \n",
            "001.txt     0      0        0      0     2       0   0      0    0    1    4   \n",
            "\n",
            "         people  said  software  technology  us  use  users  would  year  \n",
            "file                                                                      \n",
            "023.txt       0     9         0           0   0    0      0      0     0  \n",
            "020.txt       1     3         1           0   0    1      3      1     0  \n",
            "018.txt       0     5         0           2   1    2      0      1     3  \n",
            "016.txt       3     6         1           0   2    1      0      1     0  \n",
            "021.txt       3     3         0           0   0    0      1      0     0  \n",
            "...         ...   ...       ...         ...  ..  ...    ...    ...   ...  \n",
            "004.txt       0     3         0           0   2    2      0      3     2  \n",
            "003.txt       1     5         6           0   0    2      4      1     0  \n",
            "002.txt       8     3         1           0   0    4      1      0     0  \n",
            "401.txt      15     2         0           0   3    1      0      7     3  \n",
            "001.txt       0     0         0           2   1    9      0      0     0  \n",
            "\n",
            "[401 rows x 20 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consultas"
      ],
      "metadata": {
        "id": "fqOupbzf-4DU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elabore 6 exemplos de consulta."
      ],
      "metadata": {
        "id": "cNrX0KZoNsJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define queries\n",
        "queries = [\n",
        "    \"Document security and information leakage in Microsoft Word\",\n",
        "    \"Consumer Electronics Show (CES) 2005 highlights\",\n",
        "    \"Digital games and technology\",\n",
        "    \"Impact of technology on people\",\n",
        "    \"Web links and virus\",\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvSqsExDNwtH",
        "outputId": "2b753788-321b-45ed-c1f1-147fd461b57a"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document security and information leakage in Microsoft Word\n",
            "Consumer Electronics Show (CES) 2005 highlights\n",
            "Digital games and technology\n",
            "Impact of technology on people\n",
            "Web links and virus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "EPEu4w94RD6y"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeira rodada: sem retirar stopword e sem \"stemming\"\n",
        "\n"
      ],
      "metadata": {
        "id": "shnJd_JvQ4mE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo ChatGPT"
      ],
      "metadata": {
        "id": "n-VjZ1pRtqNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform your documents and queries\n",
        "tfidf_matrix = vectorizer.fit_transform(docs + queries)\n",
        "\n",
        "# Calculate the cosine similarities between the queries and documents\n",
        "cosine_similarities = cosine_similarity(tfidf_matrix[-len(queries):], tfidf_matrix[:-len(queries)])\n",
        "\n",
        "\n",
        "# Print only the titles of matching documents for each query\n",
        "for i, query in enumerate(queries):\n",
        "    sorted_indices = cosine_similarities[i].argsort()[::-1]\n",
        "    print('---' * 20)\n",
        "    print(f\"â˜† Query {i + 1}: {query} â˜†\")\n",
        "    print('---' * 20)\n",
        "    print(\"\\nMatching Document Titles:\")\n",
        "    for idx in sorted_indices:\n",
        "        document_title = docs[idx].split('\\n')[0]  # Extract the first line (title)\n",
        "        if cosine_similarities[i][idx] >  0.2:\n",
        "            print(f\"Similarity: {cosine_similarities[i][idx]:.4f} - Title: {document_title}\")\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzb1g3BZSB5V",
        "outputId": "ca4bae72-a6cf-4f5d-eb7b-6286c5f4de3a"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "â˜† Query 1: Document security and information leakage in Microsoft Word â˜†\n",
            "------------------------------------------------------------\n",
            "\n",
            "Matching Document Titles:\n",
            "Similarity: 0.3399 - Title: Warning over Windows Word files\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "â˜† Query 2: Consumer Electronics Show (CES) 2005 highlights â˜†\n",
            "------------------------------------------------------------\n",
            "\n",
            "Matching Document Titles:\n",
            "Similarity: 0.2883 - Title: Gadget market 'to grow in 2005'\n",
            "Similarity: 0.2883 - Title: Gadget market 'to grow in 2005'\n",
            "Similarity: 0.2112 - Title: Doors open at biggest gadget fair\n",
            "Similarity: 0.2112 - Title: Doors open at biggest gadget fair\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "â˜† Query 3: Digital games and technology â˜†\n",
            "------------------------------------------------------------\n",
            "\n",
            "Matching Document Titles:\n",
            "Similarity: 0.3462 - Title: Games 'deserve a place in class'\n",
            "Similarity: 0.2861 - Title: Digital UK driven by net and TV\n",
            "Similarity: 0.2458 - Title: Games enter the classroom\n",
            "Similarity: 0.2457 - Title: Parents face video game lessons\n",
            "Similarity: 0.2347 - Title: New consoles promise big problems\n",
            "Similarity: 0.2315 - Title: Casual gaming to 'take off'\n",
            "Similarity: 0.2196 - Title: Online games play with politics\n",
            "Similarity: 0.2173 - Title: Mobile games come of age\n",
            "Similarity: 0.2149 - Title: Mobile games come of age\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "â˜† Query 4: Impact of technology on people â˜†\n",
            "------------------------------------------------------------\n",
            "\n",
            "Matching Document Titles:\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "â˜† Query 5: Web links and virus â˜†\n",
            "------------------------------------------------------------\n",
            "\n",
            "Matching Document Titles:\n",
            "Similarity: 0.3536 - Title: Toxic web links help virus spread\n",
            "Similarity: 0.3191 - Title: Virus poses as Christmas e-mail\n",
            "Similarity: 0.3191 - Title: Virus poses as Christmas e-mail\n",
            "Similarity: 0.2698 - Title: Joke e-mail virus tricks users\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comentario"
      ],
      "metadata": {
        "id": "9z8sTMSq5fci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possui numero variaveis de similiariedade entre os documentos, o que afirma que os documentos possuem um nivel diferente de relevancia entre as buscas.\n",
        "\n",
        "Coloquei um nivel de similiariedade maior que 0.2 para contar como similiar. Nesse nivel a busca 4 nÃ£o possui nenhum resultado, se diminuir o nÃ­vel, aparece mais resultados.\n",
        "\n",
        "Coloquei um nivel para diminuir a quantidade que aparece, possivelmente com os stopwords e steaming a quantidade diminua.\n",
        "\n",
        "A maior similiariedade foi o documento *\"Toxic web links help virus spread\"*, para a busca 5* \"Web links and virus\"* o que faz sentido, foi desse documento que eu pequei os termos.\n"
      ],
      "metadata": {
        "id": "BizkICvk5kz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "01S9c2baxO7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcula Similiariedade do Coseno"
      ],
      "metadata": {
        "id": "3F36nVltx-cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcule_cosine_similarities(query_matrix, document_matrix):\n",
        "    cosine = cosine_similarity(query_matrix, document_matrix)\n",
        "    return cosine"
      ],
      "metadata": {
        "id": "TyfyuSeuxaCl"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print resultado da busca\n",
        "\n",
        "Considera o documento como valido dependendo do valor da similiariedade de coseno, ordena em ordem decrescente o valor de similiaredade dos documentos validos.  "
      ],
      "metadata": {
        "id": "DnCbZT_5zT-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print matching document titles and file names based on similarity threshold\n",
        "def print_similarity(queries, cosine_similarities, documents, file_names):\n",
        "    for i, query in enumerate(queries):\n",
        "        print('---' * 30)\n",
        "        print(f\"â­ Query {i + 1}: {query} â­\")\n",
        "        print('---' * 30)\n",
        "        similar_documents = []\n",
        "        for j, sim_score in enumerate(cosine_similarities[i]):\n",
        "            if sim_score > 0.1:  # Adjust the similarity threshold as needed\n",
        "                document_title = documents[j].split('\\n')[0]  # Extract the first line (title)\n",
        "                similar_documents.append((document_title, file_names[j], sim_score))\n",
        "        if similar_documents:\n",
        "            similar_documents.sort(key=lambda x: x[2], reverse=True)\n",
        "            for title, file_name, score in similar_documents:\n",
        "                print(f\"ğŸ“„ File: {file_name} âˆ½ Similarity: {score:.4f} âœ’ï¸Title: {title}\")\n",
        "        else:\n",
        "            print(\"No matching titles found.\")\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "AzsUkkrCybqt"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primeira rodada: sem retirar stopword e sem \"stemming\"\n"
      ],
      "metadata": {
        "id": "DaEC6Wpetuik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cria um objeto TfidfVectorizer chamado de **vectorizer** que converte os dados do text em um vetor **TF-IDF** (Term Frequency-Inverse Document Frequency), que Ã© uma **representaÃ§Ã£o numÃ©rica de documentos de texto**.\n",
        "\n",
        "A lista de documentos Ã© transformada em uma matriz usando o metodo de fit_trasform do vectorizer.\n",
        "\n",
        "Calcula similiariedade de cosenos entre os vetores das queries e os vetores dos documentos."
      ],
      "metadata": {
        "id": "TLZhuEo8u5cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_matching_titles(queries, documents, file_names):\n",
        "    # Create a TfidfVectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit and transform the documents to obtain TF-IDF vectors\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Transform the queries into TF-IDF vectors using the same vectorizer\n",
        "    query_tfidf_matrix = vectorizer.transform(queries)\n",
        "\n",
        "    # Calculate cosine similarities between queries and documents\n",
        "    cosine_similarities = calcule_cosine_similarities(query_tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "    print_similarity(queries, cosine_similarities, documents, file_names)\n",
        "\n",
        "# Example usage\n",
        "find_matching_titles(queries, docs, file_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu25blzdrUYH",
        "outputId": "6a972b48-81ea-4231-9ad3-825efd6a67b1"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------------------\n",
            "â­ Query 1: Document security and information leakage in Microsoft Word â­\n",
            "------------------------------------------------------------------------------------------\n",
            "ğŸ“„ File: 086.txt âˆ½ Similarity: 0.4431 âœ’ï¸Title: Warning over Windows Word files\n",
            "ğŸ“„ File: 083.txt âˆ½ Similarity: 0.1926 âœ’ï¸Title: Microsoft makes anti-piracy move\n",
            "ğŸ“„ File: 308.txt âˆ½ Similarity: 0.1926 âœ’ï¸Title: Microsoft makes anti-piracy move\n",
            "ğŸ“„ File: 036.txt âˆ½ Similarity: 0.1833 âœ’ï¸Title: Microsoft seeking spyware trojan\n",
            "ğŸ“„ File: 003.txt âˆ½ Similarity: 0.1833 âœ’ï¸Title: Microsoft seeking spyware trojan\n",
            "ğŸ“„ File: 369.txt âˆ½ Similarity: 0.1766 âœ’ï¸Title: Microsoft plans 'safer ID' system\n",
            "ğŸ“„ File: 060.txt âˆ½ Similarity: 0.1643 âœ’ï¸Title: Microsoft releases patches\n",
            "ğŸ“„ File: 007.txt âˆ½ Similarity: 0.1639 âœ’ï¸Title: Microsoft releases bumper patches\n",
            "ğŸ“„ File: 020.txt âˆ½ Similarity: 0.1587 âœ’ï¸Title: Security scares spark browser fix\n",
            "ğŸ“„ File: 151.txt âˆ½ Similarity: 0.1557 âœ’ï¸Title: 'Blog' picked as word of the year\n",
            "ğŸ“„ File: 177.txt âˆ½ Similarity: 0.1511 âœ’ï¸Title: Microsoft debuts security tools\n",
            "ğŸ“„ File: 292.txt âˆ½ Similarity: 0.1511 âœ’ï¸Title: Microsoft debuts security tools\n",
            "ğŸ“„ File: 031.txt âˆ½ Similarity: 0.1498 âœ’ï¸Title: Solutions to net security fears\n",
            "ğŸ“„ File: 250.txt âˆ½ Similarity: 0.1422 âœ’ï¸Title: Microsoft sets sights on spyware\n",
            "ğŸ“„ File: 053.txt âˆ½ Similarity: 0.1263 âœ’ï¸Title: Microsoft launches its own search\n",
            "ğŸ“„ File: 316.txt âˆ½ Similarity: 0.1116 âœ’ï¸Title: More women turn to net security\n",
            "ğŸ“„ File: 194.txt âˆ½ Similarity: 0.1098 âœ’ï¸Title: Swap offer for pirated Windows XP\n",
            "ğŸ“„ File: 034.txt âˆ½ Similarity: 0.1070 âœ’ï¸Title: UK gets official virus alert site\n",
            "ğŸ“„ File: 362.txt âˆ½ Similarity: 0.1066 âœ’ï¸Title: Bad e-mail habits sustains spam\n",
            "ğŸ“„ File: 400.txt âˆ½ Similarity: 0.1046 âœ’ï¸Title: US cyber security chief resigns\n",
            "ğŸ“„ File: 346.txt âˆ½ Similarity: 0.1020 âœ’ï¸Title: Warnings on woeful wi-fi security\n",
            "ğŸ“„ File: 227.txt âˆ½ Similarity: 0.1020 âœ’ï¸Title: Spam e-mails tempt net shoppers\n",
            "ğŸ“„ File: 398.txt âˆ½ Similarity: 0.1020 âœ’ï¸Title: Spam e-mails tempt net shoppers\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "â­ Query 2: Consumer Electronics Show (CES) 2005 highlights â­\n",
            "------------------------------------------------------------------------------------------\n",
            "ğŸ“„ File: 193.txt âˆ½ Similarity: 0.2888 âœ’ï¸Title: Gadget market 'to grow in 2005'\n",
            "ğŸ“„ File: 296.txt âˆ½ Similarity: 0.2888 âœ’ï¸Title: Gadget market 'to grow in 2005'\n",
            "ğŸ“„ File: 163.txt âˆ½ Similarity: 0.2128 âœ’ï¸Title: Doors open at biggest gadget fair\n",
            "ğŸ“„ File: 291.txt âˆ½ Similarity: 0.2128 âœ’ï¸Title: Doors open at biggest gadget fair\n",
            "ğŸ“„ File: 134.txt âˆ½ Similarity: 0.1540 âœ’ï¸Title: Gates opens biggest gadget fair\n",
            "ğŸ“„ File: 174.txt âˆ½ Similarity: 0.1492 âœ’ï¸Title: Gadgets galore on show at fair\n",
            "ğŸ“„ File: 072.txt âˆ½ Similarity: 0.1449 âœ’ï¸Title: Gadget growth fuels eco concerns\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "â­ Query 3: Digital games and technology â­\n",
            "------------------------------------------------------------------------------------------\n",
            "ğŸ“„ File: 104.txt âˆ½ Similarity: 0.3471 âœ’ï¸Title: Games 'deserve a place in class'\n",
            "ğŸ“„ File: 361.txt âˆ½ Similarity: 0.2865 âœ’ï¸Title: Digital UK driven by net and TV\n",
            "ğŸ“„ File: 231.txt âˆ½ Similarity: 0.2465 âœ’ï¸Title: Parents face video game lessons\n",
            "ğŸ“„ File: 071.txt âˆ½ Similarity: 0.2465 âœ’ï¸Title: Games enter the classroom\n",
            "ğŸ“„ File: 396.txt âˆ½ Similarity: 0.2356 âœ’ï¸Title: New consoles promise big problems\n",
            "ğŸ“„ File: 170.txt âˆ½ Similarity: 0.2322 âœ’ï¸Title: Casual gaming to 'take off'\n",
            "ğŸ“„ File: 157.txt âˆ½ Similarity: 0.2204 âœ’ï¸Title: Online games play with politics\n",
            "ğŸ“„ File: 095.txt âˆ½ Similarity: 0.2182 âœ’ï¸Title: Mobile games come of age\n",
            "ğŸ“„ File: 311.txt âˆ½ Similarity: 0.2158 âœ’ï¸Title: Mobile games come of age\n",
            "ğŸ“„ File: 203.txt âˆ½ Similarity: 0.2000 âœ’ï¸Title: Gangsters dominate gaming chart\n",
            "ğŸ“„ File: 276.txt âˆ½ Similarity: 0.1855 âœ’ï¸Title: EA to take on film and TV giants\n",
            "ğŸ“„ File: 228.txt âˆ½ Similarity: 0.1807 âœ’ï¸Title: More power to the people says HP\n",
            "ğŸ“„ File: 295.txt âˆ½ Similarity: 0.1807 âœ’ï¸Title: More power to the people says HP\n",
            "ğŸ“„ File: 155.txt âˆ½ Similarity: 0.1700 âœ’ï¸Title: Games win for Blu-ray DVD format\n",
            "ğŸ“„ File: 294.txt âˆ½ Similarity: 0.1699 âœ’ï¸Title: Games win for Blu-ray DVD format\n",
            "ğŸ“„ File: 134.txt âˆ½ Similarity: 0.1612 âœ’ï¸Title: Gates opens biggest gadget fair\n",
            "ğŸ“„ File: 082.txt âˆ½ Similarity: 0.1542 âœ’ï¸Title: Games firms 'face tough future'\n",
            "ğŸ“„ File: 163.txt âˆ½ Similarity: 0.1493 âœ’ï¸Title: Doors open at biggest gadget fair\n",
            "ğŸ“„ File: 291.txt âˆ½ Similarity: 0.1493 âœ’ï¸Title: Doors open at biggest gadget fair\n",
            "ğŸ“„ File: 306.txt âˆ½ Similarity: 0.1488 âœ’ï¸Title: Gamers could drive high-definition\n",
            "ğŸ“„ File: 100.txt âˆ½ Similarity: 0.1479 âœ’ï¸Title: Honour for UK games maker\n",
            "ğŸ“„ File: 013.txt âˆ½ Similarity: 0.1450 âœ’ï¸Title: UK pioneers digital film network\n",
            "ğŸ“„ File: 319.txt âˆ½ Similarity: 0.1393 âœ’ï¸Title: Why Cell will get the hard sell\n",
            "ğŸ“„ File: 126.txt âˆ½ Similarity: 0.1390 âœ’ï¸Title: 'Ultimate game' award for Doom 3\n",
            "ğŸ“„ File: 171.txt âˆ½ Similarity: 0.1390 âœ’ï¸Title: 'Ultimate game' award for Doom 3\n",
            "ğŸ“„ File: 211.txt âˆ½ Similarity: 0.1367 âœ’ï¸Title: Millions to miss out on the net\n",
            "ğŸ“„ File: 226.txt âˆ½ Similarity: 0.1367 âœ’ï¸Title: Millions to miss out on the net\n",
            "ğŸ“„ File: 193.txt âˆ½ Similarity: 0.1347 âœ’ï¸Title: Gadget market 'to grow in 2005'\n",
            "ğŸ“„ File: 296.txt âˆ½ Similarity: 0.1347 âœ’ï¸Title: Gadget market 'to grow in 2005'\n",
            "ğŸ“„ File: 309.txt âˆ½ Similarity: 0.1330 âœ’ï¸Title: What's next for next-gen consoles?\n",
            "ğŸ“„ File: 005.txt âˆ½ Similarity: 0.1311 âœ’ï¸Title: Technology gets the creative bug\n",
            "ğŸ“„ File: 401.txt âˆ½ Similarity: 0.1304 âœ’ï¸Title: Losing yourself in online gaming\n",
            "ğŸ“„ File: 033.txt âˆ½ Similarity: 0.1286 âœ’ï¸Title: Global digital divide 'narrowing'\n",
            "ğŸ“„ File: 135.txt âˆ½ Similarity: 0.1274 âœ’ï¸Title: GTA sequel is criminally good\n",
            "ğŸ“„ File: 056.txt âˆ½ Similarity: 0.1259 âœ’ï¸Title: Sporting rivals go to extra time\n",
            "ğŸ“„ File: 359.txt âˆ½ Similarity: 0.1250 âœ’ï¸Title: Gizmondo gadget hits the shelves\n",
            "ğŸ“„ File: 167.txt âˆ½ Similarity: 0.1238 âœ’ï¸Title: Europe backs digital TV lifestyle\n",
            "ğŸ“„ File: 215.txt âˆ½ Similarity: 0.1238 âœ’ï¸Title: Europe backs digital TV lifestyle\n",
            "ğŸ“„ File: 174.txt âˆ½ Similarity: 0.1233 âœ’ï¸Title: Gadgets galore on show at fair\n",
            "ğŸ“„ File: 084.txt âˆ½ Similarity: 0.1196 âœ’ï¸Title: Nintendo handheld given Euro date\n",
            "ğŸ“„ File: 018.txt âˆ½ Similarity: 0.1170 âœ’ï¸Title: PlayStation 3 chip to be unveiled\n",
            "ğŸ“„ File: 351.txt âˆ½ Similarity: 0.1168 âœ’ï¸Title: Nintendo DS makes its Euro debut\n",
            "ğŸ“„ File: 024.txt âˆ½ Similarity: 0.1161 âœ’ï¸Title: Game firm holds 'cast' auditions\n",
            "ğŸ“„ File: 325.txt âˆ½ Similarity: 0.1159 âœ’ï¸Title: Mobile audio enters new dimension\n",
            "ğŸ“„ File: 187.txt âˆ½ Similarity: 0.1139 âœ’ï¸Title: A question of trust and technology\n",
            "ğŸ“„ File: 115.txt âˆ½ Similarity: 0.1130 âœ’ï¸Title: Pompeii gets digital make-over\n",
            "ğŸ“„ File: 278.txt âˆ½ Similarity: 0.1117 âœ’ï¸Title: Web photo storage market hots up\n",
            "ğŸ“„ File: 279.txt âˆ½ Similarity: 0.1095 âœ’ï¸Title: Speak easy plan for media players\n",
            "ğŸ“„ File: 302.txt âˆ½ Similarity: 0.1095 âœ’ï¸Title: Speak easy plan for media players\n",
            "ğŸ“„ File: 288.txt âˆ½ Similarity: 0.1071 âœ’ï¸Title: Big war games battle it out\n",
            "ğŸ“„ File: 283.txt âˆ½ Similarity: 0.1044 âœ’ï¸Title: Games help you 'learn and play'\n",
            "ğŸ“„ File: 304.txt âˆ½ Similarity: 0.1042 âœ’ï¸Title: Format wars could 'confuse users'\n",
            "ğŸ“„ File: 285.txt âˆ½ Similarity: 0.1042 âœ’ï¸Title: Format wars could 'confuse users'\n",
            "ğŸ“„ File: 114.txt âˆ½ Similarity: 0.1040 âœ’ï¸Title: Games maker fights for survival\n",
            "ğŸ“„ File: 220.txt âˆ½ Similarity: 0.1034 âœ’ï¸Title: Britons growing 'digitally obese'\n",
            "ğŸ“„ File: 161.txt âˆ½ Similarity: 0.1028 âœ’ï¸Title: When technology gets personal\n",
            "ğŸ“„ File: 347.txt âˆ½ Similarity: 0.1024 âœ’ï¸Title: Cebit opens to mobile music tune\n",
            "ğŸ“„ File: 349.txt âˆ½ Similarity: 0.1013 âœ’ï¸Title: Broadband set to revolutionise TV\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "â­ Query 4: Impact of technology on people â­\n",
            "------------------------------------------------------------------------------------------\n",
            "ğŸ“„ File: 150.txt âˆ½ Similarity: 0.1469 âœ’ï¸Title: Broadband fuels online change\n",
            "ğŸ“„ File: 392.txt âˆ½ Similarity: 0.1467 âœ’ï¸Title: Broadband fuels online expression\n",
            "ğŸ“„ File: 325.txt âˆ½ Similarity: 0.1443 âœ’ï¸Title: Mobile audio enters new dimension\n",
            "ğŸ“„ File: 137.txt âˆ½ Similarity: 0.1297 âœ’ï¸Title: When invention turns to innovation\n",
            "ğŸ“„ File: 187.txt âˆ½ Similarity: 0.1190 âœ’ï¸Title: A question of trust and technology\n",
            "ğŸ“„ File: 228.txt âˆ½ Similarity: 0.1106 âœ’ï¸Title: More power to the people says HP\n",
            "ğŸ“„ File: 295.txt âˆ½ Similarity: 0.1106 âœ’ï¸Title: More power to the people says HP\n",
            "ğŸ“„ File: 104.txt âˆ½ Similarity: 0.1096 âœ’ï¸Title: Games 'deserve a place in class'\n",
            "ğŸ“„ File: 167.txt âˆ½ Similarity: 0.1089 âœ’ï¸Title: Europe backs digital TV lifestyle\n",
            "ğŸ“„ File: 215.txt âˆ½ Similarity: 0.1089 âœ’ï¸Title: Europe backs digital TV lifestyle\n",
            "ğŸ“„ File: 216.txt âˆ½ Similarity: 0.1054 âœ’ï¸Title: TV future in the hands of viewers\n",
            "ğŸ“„ File: 085.txt âˆ½ Similarity: 0.1039 âœ’ï¸Title: Smart search lets art fans browse\n",
            "ğŸ“„ File: 279.txt âˆ½ Similarity: 0.1033 âœ’ï¸Title: Speak easy plan for media players\n",
            "ğŸ“„ File: 302.txt âˆ½ Similarity: 0.1033 âœ’ï¸Title: Speak easy plan for media players\n",
            "ğŸ“„ File: 319.txt âˆ½ Similarity: 0.1021 âœ’ï¸Title: Why Cell will get the hard sell\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "â­ Query 5: Web links and virus â­\n",
            "------------------------------------------------------------------------------------------\n",
            "ğŸ“„ File: 210.txt âˆ½ Similarity: 0.3558 âœ’ï¸Title: Toxic web links help virus spread\n",
            "ğŸ“„ File: 008.txt âˆ½ Similarity: 0.3207 âœ’ï¸Title: Virus poses as Christmas e-mail\n",
            "ğŸ“„ File: 252.txt âˆ½ Similarity: 0.3207 âœ’ï¸Title: Virus poses as Christmas e-mail\n",
            "ğŸ“„ File: 117.txt âˆ½ Similarity: 0.2712 âœ’ï¸Title: Joke e-mail virus tricks users\n",
            "ğŸ“„ File: 281.txt âˆ½ Similarity: 0.1874 âœ’ï¸Title: Cyber crime booms in 2004\n",
            "ğŸ“„ File: 177.txt âˆ½ Similarity: 0.1864 âœ’ï¸Title: Microsoft debuts security tools\n",
            "ğŸ“„ File: 292.txt âˆ½ Similarity: 0.1864 âœ’ï¸Title: Microsoft debuts security tools\n",
            "ğŸ“„ File: 385.txt âˆ½ Similarity: 0.1539 âœ’ï¸Title: Beckham virus spotted on the net\n",
            "ğŸ“„ File: 039.txt âˆ½ Similarity: 0.1337 âœ’ï¸Title: Security warning over 'FBI virus'\n",
            "ğŸ“„ File: 334.txt âˆ½ Similarity: 0.1337 âœ’ï¸Title: Security warning over 'FBI virus'\n",
            "ğŸ“„ File: 272.txt âˆ½ Similarity: 0.1252 âœ’ï¸Title: Windows worm travels with Tetris\n",
            "ğŸ“„ File: 036.txt âˆ½ Similarity: 0.1128 âœ’ï¸Title: Microsoft seeking spyware trojan\n",
            "ğŸ“„ File: 003.txt âˆ½ Similarity: 0.1128 âœ’ï¸Title: Microsoft seeking spyware trojan\n",
            "ğŸ“„ File: 097.txt âˆ½ Similarity: 0.1105 âœ’ï¸Title: Web helps collect aid donations\n",
            "ğŸ“„ File: 092.txt âˆ½ Similarity: 0.1028 âœ’ï¸Title: Norway upholds 'Napster' ruling\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comentario:"
      ],
      "metadata": {
        "id": "8gJtY6ZG1ecE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Segunda rodada: retirando stopwords e sem \"stemming\""
      ],
      "metadata": {
        "id": "5XbHl_ql1DTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords\n"
      ],
      "metadata": {
        "id": "MpUNbXf51TOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Terceira rodada: retirando stopwords e  com \"stemming\""
      ],
      "metadata": {
        "id": "V5CphxyE1U3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conceitos"
      ],
      "metadata": {
        "id": "AmsDwmbucfYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words (BoW)\n",
        "\n",
        "The Bag of Words approach is like making a list of all the unique words you find in these books, without caring about the order or how many times each word appears. It's like counting how many times each word shows up but not worrying about the order in which they appear."
      ],
      "metadata": {
        "id": "tgZeEgiafR5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BoW = vectorizer.fit_transform(file_list)\n",
        "\n",
        "print(BoW)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDYtJhpBcgLo",
        "outputId": "76b914ea-d088-43c9-ea86-95d99e85d458"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 401)\t0.15668736734446556\n",
            "  (0, 22)\t0.9876482516132251\n",
            "  (1, 19)\t0.9876482516132251\n",
            "  (1, 401)\t0.15668736734446556\n",
            "  (2, 17)\t0.9876482516132251\n",
            "  (2, 401)\t0.15668736734446556\n",
            "  (3, 15)\t0.9876482516132251\n",
            "  (3, 401)\t0.15668736734446556\n",
            "  (4, 20)\t0.9876482516132251\n",
            "  (4, 401)\t0.15668736734446556\n",
            "  (5, 14)\t0.9876482516132251\n",
            "  (5, 401)\t0.15668736734446556\n",
            "  (6, 16)\t0.9876482516132251\n",
            "  (6, 401)\t0.15668736734446556\n",
            "  (7, 18)\t0.9876482516132251\n",
            "  (7, 401)\t0.15668736734446556\n",
            "  (8, 21)\t0.9876482516132251\n",
            "  (8, 401)\t0.15668736734446556\n",
            "  (9, 13)\t0.9876482516132251\n",
            "  (9, 401)\t0.15668736734446556\n",
            "  (10, 11)\t0.9876482516132251\n",
            "  (10, 401)\t0.15668736734446556\n",
            "  (11, 9)\t0.9876482516132251\n",
            "  (11, 401)\t0.15668736734446556\n",
            "  (12, 7)\t0.9876482516132251\n",
            "  :\t:\n",
            "  (388, 401)\t0.15668736734446556\n",
            "  (389, 371)\t0.9876482516132251\n",
            "  (389, 401)\t0.15668736734446556\n",
            "  (390, 399)\t0.9876482516132251\n",
            "  (390, 401)\t0.15668736734446556\n",
            "  (391, 394)\t0.9876482516132251\n",
            "  (391, 401)\t0.15668736734446556\n",
            "  (392, 398)\t0.9876482516132251\n",
            "  (392, 401)\t0.15668736734446556\n",
            "  (393, 395)\t0.9876482516132251\n",
            "  (393, 401)\t0.15668736734446556\n",
            "  (394, 397)\t0.9876482516132251\n",
            "  (394, 401)\t0.15668736734446556\n",
            "  (395, 396)\t0.9876482516132251\n",
            "  (395, 401)\t0.15668736734446556\n",
            "  (396, 3)\t0.9876482516132251\n",
            "  (396, 401)\t0.15668736734446556\n",
            "  (397, 2)\t0.9876482516132251\n",
            "  (397, 401)\t0.15668736734446556\n",
            "  (398, 1)\t0.9876482516132251\n",
            "  (398, 401)\t0.15668736734446556\n",
            "  (399, 400)\t0.9876482516132251\n",
            "  (399, 401)\t0.15668736734446556\n",
            "  (400, 0)\t0.9876482516132251\n",
            "  (400, 401)\t0.15668736734446556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-grams\n",
        "\n",
        "This modification will create a matrix where each column represents a bigram (a combination of two words)."
      ],
      "metadata": {
        "id": "YQzKUgjxe3KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_gramsVectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "n_grams = n_gramsVectorizer.fit_transform(file_list)\n",
        "\n",
        "print(n_grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnPnpEHRc0Lz",
        "outputId": "9755b815-66bf-49ae-9a54-a3964c86341c"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 22)\t1\n",
            "  (1, 19)\t1\n",
            "  (2, 17)\t1\n",
            "  (3, 15)\t1\n",
            "  (4, 20)\t1\n",
            "  (5, 14)\t1\n",
            "  (6, 16)\t1\n",
            "  (7, 18)\t1\n",
            "  (8, 21)\t1\n",
            "  (9, 13)\t1\n",
            "  (10, 11)\t1\n",
            "  (11, 9)\t1\n",
            "  (12, 7)\t1\n",
            "  (13, 12)\t1\n",
            "  (14, 8)\t1\n",
            "  (15, 4)\t1\n",
            "  (16, 6)\t1\n",
            "  (17, 10)\t1\n",
            "  (18, 5)\t1\n",
            "  (19, 23)\t1\n",
            "  (20, 53)\t1\n",
            "  (21, 52)\t1\n",
            "  (22, 49)\t1\n",
            "  (23, 44)\t1\n",
            "  (24, 48)\t1\n",
            "  :\t:\n",
            "  (376, 376)\t1\n",
            "  (377, 379)\t1\n",
            "  (378, 380)\t1\n",
            "  (379, 381)\t1\n",
            "  (380, 372)\t1\n",
            "  (381, 364)\t1\n",
            "  (382, 370)\t1\n",
            "  (383, 369)\t1\n",
            "  (384, 367)\t1\n",
            "  (385, 368)\t1\n",
            "  (386, 373)\t1\n",
            "  (387, 365)\t1\n",
            "  (388, 366)\t1\n",
            "  (389, 371)\t1\n",
            "  (390, 399)\t1\n",
            "  (391, 394)\t1\n",
            "  (392, 398)\t1\n",
            "  (393, 395)\t1\n",
            "  (394, 397)\t1\n",
            "  (395, 396)\t1\n",
            "  (396, 3)\t1\n",
            "  (397, 2)\t1\n",
            "  (398, 1)\t1\n",
            "  (399, 400)\t1\n",
            "  (400, 0)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HashingVectorizer\n",
        "The n_features parameter determines the number of features in the output matrix. HashingVectorizer doesn't build a vocabulary like CountVectorizer."
      ],
      "metadata": {
        "id": "RWoMcYKVe5Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "hashingV = HashingVectorizer(n_features=1000)\n",
        "\n",
        "hashing = hashingV.transform(file_list)\n",
        "\n",
        "print(hashing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfwc-RzQdkzG",
        "outputId": "4c529877-ea6f-4eb8-dfba-74ce4abc2564"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 551)\t-0.7071067811865475\n",
            "  (0, 869)\t-0.7071067811865475\n",
            "  (1, 662)\t0.7071067811865475\n",
            "  (1, 869)\t-0.7071067811865475\n",
            "  (2, 761)\t-0.7071067811865475\n",
            "  (2, 869)\t-0.7071067811865475\n",
            "  (3, 493)\t0.7071067811865475\n",
            "  (3, 869)\t-0.7071067811865475\n",
            "  (4, 716)\t-0.7071067811865475\n",
            "  (4, 869)\t-0.7071067811865475\n",
            "  (5, 832)\t-0.7071067811865475\n",
            "  (5, 869)\t-0.7071067811865475\n",
            "  (6, 402)\t-0.7071067811865475\n",
            "  (6, 869)\t-0.7071067811865475\n",
            "  (7, 15)\t-0.7071067811865475\n",
            "  (7, 869)\t-0.7071067811865475\n",
            "  (8, 241)\t0.7071067811865475\n",
            "  (8, 869)\t-0.7071067811865475\n",
            "  (9, 717)\t0.7071067811865475\n",
            "  (9, 869)\t-0.7071067811865475\n",
            "  (10, 12)\t-0.7071067811865475\n",
            "  (10, 869)\t-0.7071067811865475\n",
            "  (11, 399)\t0.7071067811865475\n",
            "  (11, 869)\t-0.7071067811865475\n",
            "  (12, 537)\t0.7071067811865475\n",
            "  :\t:\n",
            "  (388, 869)\t-0.7071067811865475\n",
            "  (389, 111)\t0.7071067811865475\n",
            "  (389, 869)\t-0.7071067811865475\n",
            "  (390, 34)\t-0.7071067811865475\n",
            "  (390, 869)\t-0.7071067811865475\n",
            "  (391, 310)\t0.7071067811865475\n",
            "  (391, 869)\t-0.7071067811865475\n",
            "  (392, 709)\t-0.7071067811865475\n",
            "  (392, 869)\t-0.7071067811865475\n",
            "  (393, 406)\t0.7071067811865475\n",
            "  (393, 869)\t-0.7071067811865475\n",
            "  (394, 686)\t-0.7071067811865475\n",
            "  (394, 869)\t-0.7071067811865475\n",
            "  (395, 149)\t-0.7071067811865475\n",
            "  (395, 869)\t-0.7071067811865475\n",
            "  (396, 283)\t0.7071067811865475\n",
            "  (396, 869)\t-0.7071067811865475\n",
            "  (397, 650)\t0.7071067811865475\n",
            "  (397, 869)\t-0.7071067811865475\n",
            "  (398, 821)\t0.7071067811865475\n",
            "  (398, 869)\t-0.7071067811865475\n",
            "  (399, 539)\t0.7071067811865475\n",
            "  (399, 869)\t-0.7071067811865475\n",
            "  (400, 16)\t-0.7071067811865475\n",
            "  (400, 869)\t-0.7071067811865475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word vs. Character N-grams\n",
        "\n",
        "You can specify whether you want word or character N-grams using the analyzer parameter. For example, to work with character bigrams (N=2) with word boundaries (like \" j\" for \"jump\"), you can do this:\n",
        "\n",
        "This will create a matrix where each column represents a character bigram with word boundaries.*italicized text*"
      ],
      "metadata": {
        "id": "5HUl2TBhfec7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams_with_boundariesV = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
        "\n",
        "bigrams = bigrams_with_boundariesV.fit_transform(file_list)\n",
        "\n",
        "print(bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u4IYd6JeA2x",
        "outputId": "89c49c4d-b3e6-4f63-a6d7-07f3904acab4"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 32)\t1\n",
            "  (0, 39)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 117)\t1\n",
            "  (0, 118)\t1\n",
            "  (0, 116)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 9)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 117)\t1\n",
            "  (1, 118)\t1\n",
            "  (1, 116)\t1\n",
            "  (1, 29)\t1\n",
            "  (1, 6)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 117)\t1\n",
            "  (2, 118)\t1\n",
            "  (2, 116)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 26)\t1\n",
            "  (2, 94)\t1\n",
            "  (3, 0)\t1\n",
            "  :\t:\n",
            "  (397, 10)\t1\n",
            "  (398, 0)\t1\n",
            "  (398, 9)\t1\n",
            "  (398, 5)\t1\n",
            "  (398, 117)\t1\n",
            "  (398, 118)\t1\n",
            "  (398, 116)\t1\n",
            "  (398, 28)\t1\n",
            "  (398, 7)\t1\n",
            "  (399, 5)\t1\n",
            "  (399, 117)\t1\n",
            "  (399, 118)\t1\n",
            "  (399, 116)\t1\n",
            "  (399, 8)\t1\n",
            "  (399, 17)\t1\n",
            "  (399, 51)\t1\n",
            "  (399, 4)\t1\n",
            "  (400, 0)\t1\n",
            "  (400, 5)\t1\n",
            "  (400, 117)\t1\n",
            "  (400, 118)\t1\n",
            "  (400, 116)\t1\n",
            "  (400, 8)\t1\n",
            "  (400, 17)\t1\n",
            "  (400, 7)\t1\n"
          ]
        }
      ]
    }
  ]
}