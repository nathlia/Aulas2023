{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "01S9c2baxO7g",
        "AmsDwmbucfYI",
        "tgZeEgiafR5z",
        "YQzKUgjxe3KN",
        "RWoMcYKVe5Sc",
        "5HUl2TBhfec7"
      ],
      "authorship_tag": "ABX9TyM76328wAccdooehfLTxUdw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathlia/Aulas2023/blob/RECUPERA%C3%87%C3%83O-DE-INFORMA%C3%87%C3%83O-E-PERSONALIZA%C3%87%C3%83O-NA-WEB-(DPADP0152)/Entrega1_nathalia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import tfidVectorizer\n"
      ],
      "metadata": {
        "id": "bZRCSsSOz_tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports e iniciando as files"
      ],
      "metadata": {
        "id": "723p12h_-veE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "y8AN6cB33OU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Use the files.upload() method to upload files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "LCzhYt973Qnk",
        "outputId": "f930ac4d-43cd-4280-8f4f-baa3d51c1a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5aacadb2-fcea-49d6-84e8-19b9461300e9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5aacadb2-fcea-49d6-84e8-19b9461300e9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dezipar(n usei)"
      ],
      "metadata": {
        "id": "ifNEaSSz0E-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the path to the zip file\n",
        "zip_file_path = '/content/drive/MyDrive/tech.zip'\n",
        "\n",
        "# Specify the directory where you want to extract the contents\n",
        "extract_dir = '/content/drive/MyDrive/tech'\n",
        "\n",
        "# Open the zip file and extract its contents\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n"
      ],
      "metadata": {
        "id": "2xEh46Y3AmcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importando os stopwords"
      ],
      "metadata": {
        "id": "OeXBYBnm0Kks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "rz1c5jo7Pdcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86949fdc-48f2-4bdb-c7d0-defa185cd60e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "## Library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import os, glob, re, sys, random, unicodedata, collections\n",
        "from tqdm import tqdm\n",
        "from functools import reduce\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "from nltk.tokenize import sent_tokenize , word_tokenize\n",
        "\n",
        "STOP_WORDS = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(STOP_WORDS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO9rP_Zu5dVG",
        "outputId": "a9871af5-9c88-4cc3-a69a-f8ce8b438663"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'once', 'off', 'when', 'and', 'with', 'are', 'into', 'below', 'herself', 'after', 'then', 'doing', \"won't\", \"you'll\", 'had', \"she's\", 'a', 'does', 'his', 'isn', 'ours', 'there', \"hadn't\", 'up', 'each', 'ma', 'until', 'what', 'being', 'not', 'same', 'over', 'hadn', 'before', 'whom', 's', \"shan't\", 'it', \"needn't\", 'am', 'at', \"haven't\", 'my', 'as', 'hers', 'their', 'should', 'needn', 'won', 'o', \"you've\", 'your', \"doesn't\", 'her', \"you'd\", 'about', 'has', 'how', 'themselves', \"couldn't\", 'both', 'other', 'some', \"isn't\", 'by', \"wouldn't\", 'if', 'these', 'on', 'doesn', \"mustn't\", 'did', 'any', 'more', 'wouldn', 'didn', 'myself', 'they', 'during', 'such', 'while', \"aren't\", 'because', 'can', 'of', 'couldn', 'yourself', 'but', 'wasn', \"it's\", \"wasn't\", 'where', 'll', 're', 'out', 'shan', 'weren', \"hasn't\", 'you', 't', 'further', 'have', 'been', 'from', 'aren', 'that', 'theirs', \"that'll\", 'which', 'ourselves', 'again', \"didn't\", 'me', 'were', 'for', 'to', 've', \"shouldn't\", 'i', 'than', 'him', 'we', 'or', 'yourselves', 'itself', 'down', 'them', 'under', 'mightn', 'an', \"weren't\", 'himself', 'so', 'most', 'why', 'will', 'do', \"don't\", 'was', 'having', 'its', 'very', 'mustn', 'd', 'be', 'all', 'those', 'who', 'no', 'now', 'ain', 'haven', 'our', 'don', 'she', 'nor', 'm', 'above', 'is', 'between', 'yours', 'in', 'through', 'against', \"should've\", 'few', 'only', \"you're\", \"mightn't\", 'y', 'he', 'shouldn', 'too', 'this', 'own', 'just', 'here', 'hasn', 'the'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_list = list(STOP_WORDS)\n",
        "print(stop_words_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NcMrFN1LoAN",
        "outputId": "c99dae69-0bc0-4733-db1f-f3e7bae212d8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['once', 'off', 'when', 'and', 'with', 'are', 'into', 'below', 'herself', 'after', 'then', 'doing', \"won't\", \"you'll\", 'had', \"she's\", 'a', 'does', 'his', 'isn', 'ours', 'there', \"hadn't\", 'up', 'each', 'ma', 'until', 'what', 'being', 'not', 'same', 'over', 'hadn', 'before', 'whom', 's', \"shan't\", 'it', \"needn't\", 'am', 'at', \"haven't\", 'my', 'as', 'hers', 'their', 'should', 'needn', 'won', 'o', \"you've\", 'your', \"doesn't\", 'her', \"you'd\", 'about', 'has', 'how', 'themselves', \"couldn't\", 'both', 'other', 'some', \"isn't\", 'by', \"wouldn't\", 'if', 'these', 'on', 'doesn', \"mustn't\", 'did', 'any', 'more', 'wouldn', 'didn', 'myself', 'they', 'during', 'such', 'while', \"aren't\", 'because', 'can', 'of', 'couldn', 'yourself', 'but', 'wasn', \"it's\", \"wasn't\", 'where', 'll', 're', 'out', 'shan', 'weren', \"hasn't\", 'you', 't', 'further', 'have', 'been', 'from', 'aren', 'that', 'theirs', \"that'll\", 'which', 'ourselves', 'again', \"didn't\", 'me', 'were', 'for', 'to', 've', \"shouldn't\", 'i', 'than', 'him', 'we', 'or', 'yourselves', 'itself', 'down', 'them', 'under', 'mightn', 'an', \"weren't\", 'himself', 'so', 'most', 'why', 'will', 'do', \"don't\", 'was', 'having', 'its', 'very', 'mustn', 'd', 'be', 'all', 'those', 'who', 'no', 'now', 'ain', 'haven', 'our', 'don', 'she', 'nor', 'm', 'above', 'is', 'between', 'yours', 'in', 'through', 'against', \"should've\", 'few', 'only', \"you're\", \"mightn't\", 'y', 'he', 'shouldn', 'too', 'this', 'own', 'just', 'here', 'hasn', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mounted google drive para acessar os textos, escolhi a pasta tech e importei para o drive"
      ],
      "metadata": {
        "id": "vmP6lNbB0Npl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRzYgZKes_gq",
        "outputId": "23c323a8-eb27-4a20-ad91-3c7f33a2e3bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to your Google Drive directory\n",
        "google_drive_path = '/content/drive/MyDrive/MyDrive/ENTREGA1/tech'\n",
        "\n",
        "# List the files and subdirectories in your Google Drive directory\n",
        "file_list = os.listdir(google_drive_path)\n",
        "\n",
        "# Count the number of files\n",
        "file_count = len(file_list)\n",
        "\n",
        "# Print the count\n",
        "print(f\"Number of files in your Google Drive directory: {file_count}\")"
      ],
      "metadata": {
        "id": "zBPnQ2JrpzEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd7f8d3-45e2-42bb-a9a9-c1164cd33612"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in your Google Drive directory: 401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n"
      ],
      "metadata": {
        "id": "931goM_P8QvB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
        "files = os.listdir(google_drive_path)  # Get all the files in that directory\n",
        "print(\"Files in %r: %s\" % (cwd, files))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT0K4QYs8gSi",
        "outputId": "d263a0b0-35c3-4b86-edeb-68921b82243c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in '/content': ['023.txt', '020.txt', '018.txt', '016.txt', '021.txt', '015.txt', '017.txt', '019.txt', '022.txt', '014.txt', '012.txt', '010.txt', '008.txt', '013.txt', '009.txt', '005.txt', '007.txt', '011.txt', '006.txt', '024.txt', '054.txt', '053.txt', '050.txt', '045.txt', '049.txt', '047.txt', '048.txt', '046.txt', '051.txt', '052.txt', '044.txt', '033.txt', '026.txt', '031.txt', '028.txt', '029.txt', '025.txt', '027.txt', '043.txt', '037.txt', '041.txt', '040.txt', '035.txt', '036.txt', '038.txt', '039.txt', '042.txt', '034.txt', '030.txt', '032.txt', '084.txt', '073.txt', '066.txt', '065.txt', '068.txt', '069.txt', '071.txt', '083.txt', '080.txt', '078.txt', '081.txt', '075.txt', '076.txt', '079.txt', '077.txt', '082.txt', '074.txt', '067.txt', '070.txt', '063.txt', '060.txt', '059.txt', '055.txt', '061.txt', '057.txt', '058.txt', '072.txt', '064.txt', '056.txt', '062.txt', '113.txt', '111.txt', '110.txt', '106.txt', '105.txt', '107.txt', '109.txt', '108.txt', '112.txt', '104.txt', '093.txt', '090.txt', '087.txt', '089.txt', '091.txt', '086.txt', '088.txt', '102.txt', '103.txt', '100.txt', '099.txt', '095.txt', '096.txt', '097.txt', '098.txt', '101.txt', '094.txt', '085.txt', '092.txt', '133.txt', '125.txt', '126.txt', '129.txt', '127.txt', '131.txt', '128.txt', '136.txt', '137.txt', '135.txt', '139.txt', '141.txt', '142.txt', '134.txt', '130.txt', '132.txt', '124.txt', '123.txt', '120.txt', '121.txt', '119.txt', '116.txt', '118.txt', '115.txt', '117.txt', '122.txt', '114.txt', '165.txt', '168.txt', '167.txt', '166.txt', '170.txt', '171.txt', '163.txt', '155.txt', '161.txt', '157.txt', '160.txt', '158.txt', '156.txt', '172.txt', '164.txt', '159.txt', '152.txt', '148.txt', '146.txt', '145.txt', '150.txt', '147.txt', '149.txt', '162.txt', '154.txt', '151.txt', '153.txt', '143.txt', '140.txt', '138.txt', '144.txt', '200.txt', '202.txt', '194.txt', '193.txt', '188.txt', '189.txt', '191.txt', '185.txt', '190.txt', '186.txt', '187.txt', '192.txt', '183.txt', '177.txt', '178.txt', '181.txt', '179.txt', '175.txt', '180.txt', '184.txt', '176.txt', '182.txt', '174.txt', '173.txt', '169.txt', '232.txt', '203.txt', '195.txt', '201.txt', '199.txt', '196.txt', '197.txt', '198.txt', '224.txt', '223.txt', '216.txt', '220.txt', '219.txt', '218.txt', '221.txt', '217.txt', '213.txt', '210.txt', '207.txt', '208.txt', '209.txt', '205.txt', '211.txt', '215.txt', '222.txt', '214.txt', '206.txt', '212.txt', '204.txt', '254.txt', '243.txt', '238.txt', '235.txt', '237.txt', '240.txt', '241.txt', '236.txt', '253.txt', '249.txt', '250.txt', '246.txt', '248.txt', '247.txt', '251.txt', '245.txt', '252.txt', '244.txt', '242.txt', '239.txt', '234.txt', '233.txt', '230.txt', '228.txt', '225.txt', '227.txt', '229.txt', '231.txt', '226.txt', '284.txt', '283.txt', '277.txt', '278.txt', '275.txt', '281.txt', '280.txt', '276.txt', '273.txt', '267.txt', '266.txt', '265.txt', '271.txt', '270.txt', '268.txt', '279.txt', '282.txt', '272.txt', '274.txt', '263.txt', '259.txt', '260.txt', '257.txt', '258.txt', '256.txt', '255.txt', '269.txt', '264.txt', '261.txt', '262.txt', '313.txt', '311.txt', '305.txt', '309.txt', '306.txt', '307.txt', '310.txt', '303.txt', '296.txt', '295.txt', '299.txt', '301.txt', '298.txt', '297.txt', '308.txt', '312.txt', '304.txt', '293.txt', '288.txt', '287.txt', '286.txt', '285.txt', '290.txt', '291.txt', '300.txt', '302.txt', '294.txt', '289.txt', '292.txt', '339.txt', '336.txt', '342.txt', '334.txt', '333.txt', '331.txt', '325.txt', '329.txt', '330.txt', '326.txt', '327.txt', '324.txt', '328.txt', '323.txt', '321.txt', '320.txt', '319.txt', '318.txt', '317.txt', '316.txt', '332.txt', '315.txt', '322.txt', '314.txt', '364.txt', '363.txt', '357.txt', '358.txt', '360.txt', '356.txt', '355.txt', '361.txt', '353.txt', '350.txt', '348.txt', '346.txt', '345.txt', '349.txt', '351.txt', '359.txt', '362.txt', '354.txt', '347.txt', '343.txt', '335.txt', '341.txt', '340.txt', '338.txt', '337.txt', '352.txt', '344.txt', '394.txt', '393.txt', '389.txt', '390.txt', '386.txt', '391.txt', '385.txt', '387.txt', '388.txt', '392.txt', '384.txt', '383.txt', '379.txt', '378.txt', '375.txt', '376.txt', '377.txt', '380.txt', '381.txt', '382.txt', '373.txt', '365.txt', '371.txt', '370.txt', '368.txt', '369.txt', '374.txt', '366.txt', '367.txt', '372.txt', '400.txt', '395.txt', '399.txt', '396.txt', '398.txt', '397.txt', '004.txt', '003.txt', '002.txt', '401.txt', '001.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select and read 10 random files\n",
        "sample_texts = files\n",
        "# sample_texts = random.sample(file_list, 100)\n",
        "print(\"Lista de textos:\\n\", sample_texts)\n",
        "\n",
        "docs = []\n",
        "for fname in sample_texts:\n",
        "    with open(google_drive_path + '/' + fname , \"r\", encoding=\"ISO-8859-1\") as file:\n",
        "        text = file.read()\n",
        "    docs.append(text)\n",
        "\n",
        "print(\"Finished reading files in:\", google_drive_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTqF3Rv82opb",
        "outputId": "1cc46704-e3a8-4902-b998-9ae819d84795"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lista de textos:\n",
            " ['023.txt', '020.txt', '018.txt', '016.txt', '021.txt', '015.txt', '017.txt', '019.txt', '022.txt', '014.txt', '012.txt', '010.txt', '008.txt', '013.txt', '009.txt', '005.txt', '007.txt', '011.txt', '006.txt', '024.txt', '054.txt', '053.txt', '050.txt', '045.txt', '049.txt', '047.txt', '048.txt', '046.txt', '051.txt', '052.txt', '044.txt', '033.txt', '026.txt', '031.txt', '028.txt', '029.txt', '025.txt', '027.txt', '043.txt', '037.txt', '041.txt', '040.txt', '035.txt', '036.txt', '038.txt', '039.txt', '042.txt', '034.txt', '030.txt', '032.txt', '084.txt', '073.txt', '066.txt', '065.txt', '068.txt', '069.txt', '071.txt', '083.txt', '080.txt', '078.txt', '081.txt', '075.txt', '076.txt', '079.txt', '077.txt', '082.txt', '074.txt', '067.txt', '070.txt', '063.txt', '060.txt', '059.txt', '055.txt', '061.txt', '057.txt', '058.txt', '072.txt', '064.txt', '056.txt', '062.txt', '113.txt', '111.txt', '110.txt', '106.txt', '105.txt', '107.txt', '109.txt', '108.txt', '112.txt', '104.txt', '093.txt', '090.txt', '087.txt', '089.txt', '091.txt', '086.txt', '088.txt', '102.txt', '103.txt', '100.txt', '099.txt', '095.txt', '096.txt', '097.txt', '098.txt', '101.txt', '094.txt', '085.txt', '092.txt', '133.txt', '125.txt', '126.txt', '129.txt', '127.txt', '131.txt', '128.txt', '136.txt', '137.txt', '135.txt', '139.txt', '141.txt', '142.txt', '134.txt', '130.txt', '132.txt', '124.txt', '123.txt', '120.txt', '121.txt', '119.txt', '116.txt', '118.txt', '115.txt', '117.txt', '122.txt', '114.txt', '165.txt', '168.txt', '167.txt', '166.txt', '170.txt', '171.txt', '163.txt', '155.txt', '161.txt', '157.txt', '160.txt', '158.txt', '156.txt', '172.txt', '164.txt', '159.txt', '152.txt', '148.txt', '146.txt', '145.txt', '150.txt', '147.txt', '149.txt', '162.txt', '154.txt', '151.txt', '153.txt', '143.txt', '140.txt', '138.txt', '144.txt', '200.txt', '202.txt', '194.txt', '193.txt', '188.txt', '189.txt', '191.txt', '185.txt', '190.txt', '186.txt', '187.txt', '192.txt', '183.txt', '177.txt', '178.txt', '181.txt', '179.txt', '175.txt', '180.txt', '184.txt', '176.txt', '182.txt', '174.txt', '173.txt', '169.txt', '232.txt', '203.txt', '195.txt', '201.txt', '199.txt', '196.txt', '197.txt', '198.txt', '224.txt', '223.txt', '216.txt', '220.txt', '219.txt', '218.txt', '221.txt', '217.txt', '213.txt', '210.txt', '207.txt', '208.txt', '209.txt', '205.txt', '211.txt', '215.txt', '222.txt', '214.txt', '206.txt', '212.txt', '204.txt', '254.txt', '243.txt', '238.txt', '235.txt', '237.txt', '240.txt', '241.txt', '236.txt', '253.txt', '249.txt', '250.txt', '246.txt', '248.txt', '247.txt', '251.txt', '245.txt', '252.txt', '244.txt', '242.txt', '239.txt', '234.txt', '233.txt', '230.txt', '228.txt', '225.txt', '227.txt', '229.txt', '231.txt', '226.txt', '284.txt', '283.txt', '277.txt', '278.txt', '275.txt', '281.txt', '280.txt', '276.txt', '273.txt', '267.txt', '266.txt', '265.txt', '271.txt', '270.txt', '268.txt', '279.txt', '282.txt', '272.txt', '274.txt', '263.txt', '259.txt', '260.txt', '257.txt', '258.txt', '256.txt', '255.txt', '269.txt', '264.txt', '261.txt', '262.txt', '313.txt', '311.txt', '305.txt', '309.txt', '306.txt', '307.txt', '310.txt', '303.txt', '296.txt', '295.txt', '299.txt', '301.txt', '298.txt', '297.txt', '308.txt', '312.txt', '304.txt', '293.txt', '288.txt', '287.txt', '286.txt', '285.txt', '290.txt', '291.txt', '300.txt', '302.txt', '294.txt', '289.txt', '292.txt', '339.txt', '336.txt', '342.txt', '334.txt', '333.txt', '331.txt', '325.txt', '329.txt', '330.txt', '326.txt', '327.txt', '324.txt', '328.txt', '323.txt', '321.txt', '320.txt', '319.txt', '318.txt', '317.txt', '316.txt', '332.txt', '315.txt', '322.txt', '314.txt', '364.txt', '363.txt', '357.txt', '358.txt', '360.txt', '356.txt', '355.txt', '361.txt', '353.txt', '350.txt', '348.txt', '346.txt', '345.txt', '349.txt', '351.txt', '359.txt', '362.txt', '354.txt', '347.txt', '343.txt', '335.txt', '341.txt', '340.txt', '338.txt', '337.txt', '352.txt', '344.txt', '394.txt', '393.txt', '389.txt', '390.txt', '386.txt', '391.txt', '385.txt', '387.txt', '388.txt', '392.txt', '384.txt', '383.txt', '379.txt', '378.txt', '375.txt', '376.txt', '377.txt', '380.txt', '381.txt', '382.txt', '373.txt', '365.txt', '371.txt', '370.txt', '368.txt', '369.txt', '374.txt', '366.txt', '367.txt', '372.txt', '400.txt', '395.txt', '399.txt', '396.txt', '398.txt', '397.txt', '004.txt', '003.txt', '002.txt', '401.txt', '001.txt']\n",
            "Finished reading files in: /content/drive/MyDrive/MyDrive/ENTREGA1/tech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count term frequency using CountVectorizer from scikit-learn\n",
        "## limiting number of words just for illustrating the concept\n",
        "\n",
        "vec = CountVectorizer(max_features=20, stop_words=stop_words_list)\n",
        "X = vec.fit_transform(docs)\n",
        "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())\n",
        "files_names = [file.split('/')[-1] for file in sample_texts]\n",
        "df['file'] = files_names\n",
        "df = df.set_index('file')\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR0REeZ0KEcl",
        "outputId": "8a761047-9802-4113-e8ca-977d86f1d2a0"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         also  could  digital  games  many  mobile  mr  music  net  new  one  \\\n",
            "file                                                                           \n",
            "023.txt     2      0        0      0     0       0   3      0    0    0    0   \n",
            "020.txt     2      0        0      0     1       0   1      0    0    6    1   \n",
            "018.txt     2      3        1      3     0       0   0      0    0    1    0   \n",
            "016.txt     0      2        0      0     1       0   5      0    0    1    6   \n",
            "021.txt     0      1        0      0     2       0   1      0    1    1    1   \n",
            "...       ...    ...      ...    ...   ...     ...  ..    ...  ...  ...  ...   \n",
            "004.txt     1      2        4      1     0       1   4      0    0    0    3   \n",
            "003.txt     1      0        0      0     0       0   0      0    0    0    0   \n",
            "002.txt     3      0        0      1     1       0   0      0   15    1    1   \n",
            "401.txt     0      5        0     22     4       0   0      0    0    2    7   \n",
            "001.txt     0      0        0      0     2       0   0      0    0    1    4   \n",
            "\n",
            "         people  said  software  technology  us  use  users  would  year  \n",
            "file                                                                      \n",
            "023.txt       0     9         0           0   0    0      0      0     0  \n",
            "020.txt       1     3         1           0   0    1      3      1     0  \n",
            "018.txt       0     5         0           2   1    2      0      1     3  \n",
            "016.txt       3     6         1           0   2    1      0      1     0  \n",
            "021.txt       3     3         0           0   0    0      1      0     0  \n",
            "...         ...   ...       ...         ...  ..  ...    ...    ...   ...  \n",
            "004.txt       0     3         0           0   2    2      0      3     2  \n",
            "003.txt       1     5         6           0   0    2      4      1     0  \n",
            "002.txt       8     3         1           0   0    4      1      0     0  \n",
            "401.txt      15     2         0           0   3    1      0      7     3  \n",
            "001.txt       0     0         0           2   1    9      0      0     0  \n",
            "\n",
            "[401 rows x 20 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consultas"
      ],
      "metadata": {
        "id": "fqOupbzf-4DU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elabore 6 exemplos de consulta."
      ],
      "metadata": {
        "id": "cNrX0KZoNsJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define queries\n",
        "queries = [\n",
        "    \"Document security and information leakage in Microsoft Word\",\n",
        "    \"Consumer Electronics Show (CES) 2005 highlights\",\n",
        "    \"Digital games and technology\",\n",
        "    \"Impact of technology on people\",\n",
        "    \"Web links and virus\",\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvSqsExDNwtH",
        "outputId": "2b753788-321b-45ed-c1f1-147fd461b57a"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document security and information leakage in Microsoft Word\n",
            "Consumer Electronics Show (CES) 2005 highlights\n",
            "Digital games and technology\n",
            "Impact of technology on people\n",
            "Web links and virus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "EPEu4w94RD6y"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeira rodada: sem retirar stopword e sem \"stemming\"\n",
        "\n"
      ],
      "metadata": {
        "id": "shnJd_JvQ4mE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo ChatGPT"
      ],
      "metadata": {
        "id": "n-VjZ1pRtqNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform your documents and queries\n",
        "tfidf_matrix = vectorizer.fit_transform(docs + queries)\n",
        "\n",
        "# Calculate the cosine similarities between the queries and documents\n",
        "cosine_similarities = cosine_similarity(tfidf_matrix[-len(queries):], tfidf_matrix[:-len(queries)])\n",
        "\n",
        "\n",
        "# Print only the titles of matching documents for each query\n",
        "for i, query in enumerate(queries):\n",
        "    sorted_indices = cosine_similarities[i].argsort()[::-1]\n",
        "    print('---' * 20)\n",
        "    print(f\"☆ Query {i + 1}: {query} ☆\")\n",
        "    print('---' * 20)\n",
        "    print(\"\\nMatching Document Titles:\")\n",
        "    for idx in sorted_indices:\n",
        "        document_title = docs[idx].split('\\n')[0]  # Extract the first line (title)\n",
        "        if cosine_similarities[i][idx] >  0.2:\n",
        "            print(f\"Similarity: {cosine_similarities[i][idx]:.4f} - Title: {document_title}\")\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzb1g3BZSB5V",
        "outputId": "ca4bae72-a6cf-4f5d-eb7b-6286c5f4de3a"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "☆ Query 1: Document security and information leakage in Microsoft Word ☆\n",
            "------------------------------------------------------------\n",
            "\n",
            "Matching Document Titles:\n",
            "Similarity: 0.3399 - Title: Warning over Windows Word files\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "☆ Query 2: Consumer Electronics Show (CES) 2005 highlights ☆\n",
            "------------------------------------------------------------\n",
            "\n",
            "Matching Document Titles:\n",
            "Similarity: 0.2883 - Title: Gadget market 'to grow in 2005'\n",
            "Similarity: 0.2883 - Title: Gadget market 'to grow in 2005'\n",
            "Similarity: 0.2112 - Title: Doors open at biggest gadget fair\n",
            "Similarity: 0.2112 - Title: Doors open at biggest gadget fair\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "☆ Query 3: Digital games and technology ☆\n",
            "------------------------------------------------------------\n",
            "\n",
            "Matching Document Titles:\n",
            "Similarity: 0.3462 - Title: Games 'deserve a place in class'\n",
            "Similarity: 0.2861 - Title: Digital UK driven by net and TV\n",
            "Similarity: 0.2458 - Title: Games enter the classroom\n",
            "Similarity: 0.2457 - Title: Parents face video game lessons\n",
            "Similarity: 0.2347 - Title: New consoles promise big problems\n",
            "Similarity: 0.2315 - Title: Casual gaming to 'take off'\n",
            "Similarity: 0.2196 - Title: Online games play with politics\n",
            "Similarity: 0.2173 - Title: Mobile games come of age\n",
            "Similarity: 0.2149 - Title: Mobile games come of age\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "☆ Query 4: Impact of technology on people ☆\n",
            "------------------------------------------------------------\n",
            "\n",
            "Matching Document Titles:\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "☆ Query 5: Web links and virus ☆\n",
            "------------------------------------------------------------\n",
            "\n",
            "Matching Document Titles:\n",
            "Similarity: 0.3536 - Title: Toxic web links help virus spread\n",
            "Similarity: 0.3191 - Title: Virus poses as Christmas e-mail\n",
            "Similarity: 0.3191 - Title: Virus poses as Christmas e-mail\n",
            "Similarity: 0.2698 - Title: Joke e-mail virus tricks users\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comentario"
      ],
      "metadata": {
        "id": "9z8sTMSq5fci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possui numero variaveis de similiariedade entre os documentos, o que afirma que os documentos possuem um nivel diferente de relevancia entre as buscas.\n",
        "\n",
        "Coloquei um nivel de similiariedade maior que 0.2 para contar como similiar. Nesse nivel a busca 4 não possui nenhum resultado, se diminuir o nível, aparece mais resultados.\n",
        "\n",
        "Coloquei um nivel para diminuir a quantidade que aparece, possivelmente com os stopwords e steaming a quantidade diminua.\n",
        "\n",
        "A maior similiariedade foi o documento *\"Toxic web links help virus spread\"*, para a busca 5* \"Web links and virus\"* o que faz sentido, foi desse documento que eu pequei os termos.\n"
      ],
      "metadata": {
        "id": "BizkICvk5kz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "01S9c2baxO7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcula Similiariedade do Coseno"
      ],
      "metadata": {
        "id": "3F36nVltx-cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcule_cosine_similarities(query_matrix, document_matrix):\n",
        "    cosine = cosine_similarity(query_matrix, document_matrix)\n",
        "    return cosine"
      ],
      "metadata": {
        "id": "TyfyuSeuxaCl"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print resultado da busca\n",
        "\n",
        "Considera o documento como valido dependendo do valor da similiariedade de coseno, ordena em ordem decrescente o valor de similiaredade dos documentos validos.  "
      ],
      "metadata": {
        "id": "DnCbZT_5zT-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print matching document titles and file names based on similarity threshold\n",
        "def print_similarity(queries, cosine_similarities, documents, file_names):\n",
        "    for i, query in enumerate(queries):\n",
        "        print('---' * 30)\n",
        "        print(f\"⭐ Query {i + 1}: {query} ⭐\")\n",
        "        print('---' * 30)\n",
        "        similar_documents = []\n",
        "        for j, sim_score in enumerate(cosine_similarities[i]):\n",
        "            if sim_score > 0.1:  # Adjust the similarity threshold as needed\n",
        "                document_title = documents[j].split('\\n')[0]  # Extract the first line (title)\n",
        "                similar_documents.append((document_title, file_names[j], sim_score))\n",
        "        if similar_documents:\n",
        "            similar_documents.sort(key=lambda x: x[2], reverse=True)\n",
        "            for title, file_name, score in similar_documents:\n",
        "                print(f\"📄 File: {file_name} ∽ Similarity: {score:.4f} ✒️Title: {title}\")\n",
        "        else:\n",
        "            print(\"No matching titles found.\")\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "AzsUkkrCybqt"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primeira rodada: sem retirar stopword e sem \"stemming\"\n"
      ],
      "metadata": {
        "id": "DaEC6Wpetuik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cria um objeto TfidfVectorizer chamado de **vectorizer** que converte os dados do text em um vetor **TF-IDF** (Term Frequency-Inverse Document Frequency), que é uma **representação numérica de documentos de texto**.\n",
        "\n",
        "A lista de documentos é transformada em uma matriz usando o metodo de fit_trasform do vectorizer.\n",
        "\n",
        "Calcula similiariedade de cosenos entre os vetores das queries e os vetores dos documentos."
      ],
      "metadata": {
        "id": "TLZhuEo8u5cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_matching_titles(queries, documents, file_names):\n",
        "    # Create a TfidfVectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit and transform the documents to obtain TF-IDF vectors\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Transform the queries into TF-IDF vectors using the same vectorizer\n",
        "    query_tfidf_matrix = vectorizer.transform(queries)\n",
        "\n",
        "    # Calculate cosine similarities between queries and documents\n",
        "    cosine_similarities = calcule_cosine_similarities(query_tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "    print_similarity(queries, cosine_similarities, documents, file_names)\n",
        "\n",
        "# Example usage\n",
        "find_matching_titles(queries, docs, file_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu25blzdrUYH",
        "outputId": "6a972b48-81ea-4231-9ad3-825efd6a67b1"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------------------\n",
            "⭐ Query 1: Document security and information leakage in Microsoft Word ⭐\n",
            "------------------------------------------------------------------------------------------\n",
            "📄 File: 086.txt ∽ Similarity: 0.4431 ✒️Title: Warning over Windows Word files\n",
            "📄 File: 083.txt ∽ Similarity: 0.1926 ✒️Title: Microsoft makes anti-piracy move\n",
            "📄 File: 308.txt ∽ Similarity: 0.1926 ✒️Title: Microsoft makes anti-piracy move\n",
            "📄 File: 036.txt ∽ Similarity: 0.1833 ✒️Title: Microsoft seeking spyware trojan\n",
            "📄 File: 003.txt ∽ Similarity: 0.1833 ✒️Title: Microsoft seeking spyware trojan\n",
            "📄 File: 369.txt ∽ Similarity: 0.1766 ✒️Title: Microsoft plans 'safer ID' system\n",
            "📄 File: 060.txt ∽ Similarity: 0.1643 ✒️Title: Microsoft releases patches\n",
            "📄 File: 007.txt ∽ Similarity: 0.1639 ✒️Title: Microsoft releases bumper patches\n",
            "📄 File: 020.txt ∽ Similarity: 0.1587 ✒️Title: Security scares spark browser fix\n",
            "📄 File: 151.txt ∽ Similarity: 0.1557 ✒️Title: 'Blog' picked as word of the year\n",
            "📄 File: 177.txt ∽ Similarity: 0.1511 ✒️Title: Microsoft debuts security tools\n",
            "📄 File: 292.txt ∽ Similarity: 0.1511 ✒️Title: Microsoft debuts security tools\n",
            "📄 File: 031.txt ∽ Similarity: 0.1498 ✒️Title: Solutions to net security fears\n",
            "📄 File: 250.txt ∽ Similarity: 0.1422 ✒️Title: Microsoft sets sights on spyware\n",
            "📄 File: 053.txt ∽ Similarity: 0.1263 ✒️Title: Microsoft launches its own search\n",
            "📄 File: 316.txt ∽ Similarity: 0.1116 ✒️Title: More women turn to net security\n",
            "📄 File: 194.txt ∽ Similarity: 0.1098 ✒️Title: Swap offer for pirated Windows XP\n",
            "📄 File: 034.txt ∽ Similarity: 0.1070 ✒️Title: UK gets official virus alert site\n",
            "📄 File: 362.txt ∽ Similarity: 0.1066 ✒️Title: Bad e-mail habits sustains spam\n",
            "📄 File: 400.txt ∽ Similarity: 0.1046 ✒️Title: US cyber security chief resigns\n",
            "📄 File: 346.txt ∽ Similarity: 0.1020 ✒️Title: Warnings on woeful wi-fi security\n",
            "📄 File: 227.txt ∽ Similarity: 0.1020 ✒️Title: Spam e-mails tempt net shoppers\n",
            "📄 File: 398.txt ∽ Similarity: 0.1020 ✒️Title: Spam e-mails tempt net shoppers\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "⭐ Query 2: Consumer Electronics Show (CES) 2005 highlights ⭐\n",
            "------------------------------------------------------------------------------------------\n",
            "📄 File: 193.txt ∽ Similarity: 0.2888 ✒️Title: Gadget market 'to grow in 2005'\n",
            "📄 File: 296.txt ∽ Similarity: 0.2888 ✒️Title: Gadget market 'to grow in 2005'\n",
            "📄 File: 163.txt ∽ Similarity: 0.2128 ✒️Title: Doors open at biggest gadget fair\n",
            "📄 File: 291.txt ∽ Similarity: 0.2128 ✒️Title: Doors open at biggest gadget fair\n",
            "📄 File: 134.txt ∽ Similarity: 0.1540 ✒️Title: Gates opens biggest gadget fair\n",
            "📄 File: 174.txt ∽ Similarity: 0.1492 ✒️Title: Gadgets galore on show at fair\n",
            "📄 File: 072.txt ∽ Similarity: 0.1449 ✒️Title: Gadget growth fuels eco concerns\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "⭐ Query 3: Digital games and technology ⭐\n",
            "------------------------------------------------------------------------------------------\n",
            "📄 File: 104.txt ∽ Similarity: 0.3471 ✒️Title: Games 'deserve a place in class'\n",
            "📄 File: 361.txt ∽ Similarity: 0.2865 ✒️Title: Digital UK driven by net and TV\n",
            "📄 File: 231.txt ∽ Similarity: 0.2465 ✒️Title: Parents face video game lessons\n",
            "📄 File: 071.txt ∽ Similarity: 0.2465 ✒️Title: Games enter the classroom\n",
            "📄 File: 396.txt ∽ Similarity: 0.2356 ✒️Title: New consoles promise big problems\n",
            "📄 File: 170.txt ∽ Similarity: 0.2322 ✒️Title: Casual gaming to 'take off'\n",
            "📄 File: 157.txt ∽ Similarity: 0.2204 ✒️Title: Online games play with politics\n",
            "📄 File: 095.txt ∽ Similarity: 0.2182 ✒️Title: Mobile games come of age\n",
            "📄 File: 311.txt ∽ Similarity: 0.2158 ✒️Title: Mobile games come of age\n",
            "📄 File: 203.txt ∽ Similarity: 0.2000 ✒️Title: Gangsters dominate gaming chart\n",
            "📄 File: 276.txt ∽ Similarity: 0.1855 ✒️Title: EA to take on film and TV giants\n",
            "📄 File: 228.txt ∽ Similarity: 0.1807 ✒️Title: More power to the people says HP\n",
            "📄 File: 295.txt ∽ Similarity: 0.1807 ✒️Title: More power to the people says HP\n",
            "📄 File: 155.txt ∽ Similarity: 0.1700 ✒️Title: Games win for Blu-ray DVD format\n",
            "📄 File: 294.txt ∽ Similarity: 0.1699 ✒️Title: Games win for Blu-ray DVD format\n",
            "📄 File: 134.txt ∽ Similarity: 0.1612 ✒️Title: Gates opens biggest gadget fair\n",
            "📄 File: 082.txt ∽ Similarity: 0.1542 ✒️Title: Games firms 'face tough future'\n",
            "📄 File: 163.txt ∽ Similarity: 0.1493 ✒️Title: Doors open at biggest gadget fair\n",
            "📄 File: 291.txt ∽ Similarity: 0.1493 ✒️Title: Doors open at biggest gadget fair\n",
            "📄 File: 306.txt ∽ Similarity: 0.1488 ✒️Title: Gamers could drive high-definition\n",
            "📄 File: 100.txt ∽ Similarity: 0.1479 ✒️Title: Honour for UK games maker\n",
            "📄 File: 013.txt ∽ Similarity: 0.1450 ✒️Title: UK pioneers digital film network\n",
            "📄 File: 319.txt ∽ Similarity: 0.1393 ✒️Title: Why Cell will get the hard sell\n",
            "📄 File: 126.txt ∽ Similarity: 0.1390 ✒️Title: 'Ultimate game' award for Doom 3\n",
            "📄 File: 171.txt ∽ Similarity: 0.1390 ✒️Title: 'Ultimate game' award for Doom 3\n",
            "📄 File: 211.txt ∽ Similarity: 0.1367 ✒️Title: Millions to miss out on the net\n",
            "📄 File: 226.txt ∽ Similarity: 0.1367 ✒️Title: Millions to miss out on the net\n",
            "📄 File: 193.txt ∽ Similarity: 0.1347 ✒️Title: Gadget market 'to grow in 2005'\n",
            "📄 File: 296.txt ∽ Similarity: 0.1347 ✒️Title: Gadget market 'to grow in 2005'\n",
            "📄 File: 309.txt ∽ Similarity: 0.1330 ✒️Title: What's next for next-gen consoles?\n",
            "📄 File: 005.txt ∽ Similarity: 0.1311 ✒️Title: Technology gets the creative bug\n",
            "📄 File: 401.txt ∽ Similarity: 0.1304 ✒️Title: Losing yourself in online gaming\n",
            "📄 File: 033.txt ∽ Similarity: 0.1286 ✒️Title: Global digital divide 'narrowing'\n",
            "📄 File: 135.txt ∽ Similarity: 0.1274 ✒️Title: GTA sequel is criminally good\n",
            "📄 File: 056.txt ∽ Similarity: 0.1259 ✒️Title: Sporting rivals go to extra time\n",
            "📄 File: 359.txt ∽ Similarity: 0.1250 ✒️Title: Gizmondo gadget hits the shelves\n",
            "📄 File: 167.txt ∽ Similarity: 0.1238 ✒️Title: Europe backs digital TV lifestyle\n",
            "📄 File: 215.txt ∽ Similarity: 0.1238 ✒️Title: Europe backs digital TV lifestyle\n",
            "📄 File: 174.txt ∽ Similarity: 0.1233 ✒️Title: Gadgets galore on show at fair\n",
            "📄 File: 084.txt ∽ Similarity: 0.1196 ✒️Title: Nintendo handheld given Euro date\n",
            "📄 File: 018.txt ∽ Similarity: 0.1170 ✒️Title: PlayStation 3 chip to be unveiled\n",
            "📄 File: 351.txt ∽ Similarity: 0.1168 ✒️Title: Nintendo DS makes its Euro debut\n",
            "📄 File: 024.txt ∽ Similarity: 0.1161 ✒️Title: Game firm holds 'cast' auditions\n",
            "📄 File: 325.txt ∽ Similarity: 0.1159 ✒️Title: Mobile audio enters new dimension\n",
            "📄 File: 187.txt ∽ Similarity: 0.1139 ✒️Title: A question of trust and technology\n",
            "📄 File: 115.txt ∽ Similarity: 0.1130 ✒️Title: Pompeii gets digital make-over\n",
            "📄 File: 278.txt ∽ Similarity: 0.1117 ✒️Title: Web photo storage market hots up\n",
            "📄 File: 279.txt ∽ Similarity: 0.1095 ✒️Title: Speak easy plan for media players\n",
            "📄 File: 302.txt ∽ Similarity: 0.1095 ✒️Title: Speak easy plan for media players\n",
            "📄 File: 288.txt ∽ Similarity: 0.1071 ✒️Title: Big war games battle it out\n",
            "📄 File: 283.txt ∽ Similarity: 0.1044 ✒️Title: Games help you 'learn and play'\n",
            "📄 File: 304.txt ∽ Similarity: 0.1042 ✒️Title: Format wars could 'confuse users'\n",
            "📄 File: 285.txt ∽ Similarity: 0.1042 ✒️Title: Format wars could 'confuse users'\n",
            "📄 File: 114.txt ∽ Similarity: 0.1040 ✒️Title: Games maker fights for survival\n",
            "📄 File: 220.txt ∽ Similarity: 0.1034 ✒️Title: Britons growing 'digitally obese'\n",
            "📄 File: 161.txt ∽ Similarity: 0.1028 ✒️Title: When technology gets personal\n",
            "📄 File: 347.txt ∽ Similarity: 0.1024 ✒️Title: Cebit opens to mobile music tune\n",
            "📄 File: 349.txt ∽ Similarity: 0.1013 ✒️Title: Broadband set to revolutionise TV\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "⭐ Query 4: Impact of technology on people ⭐\n",
            "------------------------------------------------------------------------------------------\n",
            "📄 File: 150.txt ∽ Similarity: 0.1469 ✒️Title: Broadband fuels online change\n",
            "📄 File: 392.txt ∽ Similarity: 0.1467 ✒️Title: Broadband fuels online expression\n",
            "📄 File: 325.txt ∽ Similarity: 0.1443 ✒️Title: Mobile audio enters new dimension\n",
            "📄 File: 137.txt ∽ Similarity: 0.1297 ✒️Title: When invention turns to innovation\n",
            "📄 File: 187.txt ∽ Similarity: 0.1190 ✒️Title: A question of trust and technology\n",
            "📄 File: 228.txt ∽ Similarity: 0.1106 ✒️Title: More power to the people says HP\n",
            "📄 File: 295.txt ∽ Similarity: 0.1106 ✒️Title: More power to the people says HP\n",
            "📄 File: 104.txt ∽ Similarity: 0.1096 ✒️Title: Games 'deserve a place in class'\n",
            "📄 File: 167.txt ∽ Similarity: 0.1089 ✒️Title: Europe backs digital TV lifestyle\n",
            "📄 File: 215.txt ∽ Similarity: 0.1089 ✒️Title: Europe backs digital TV lifestyle\n",
            "📄 File: 216.txt ∽ Similarity: 0.1054 ✒️Title: TV future in the hands of viewers\n",
            "📄 File: 085.txt ∽ Similarity: 0.1039 ✒️Title: Smart search lets art fans browse\n",
            "📄 File: 279.txt ∽ Similarity: 0.1033 ✒️Title: Speak easy plan for media players\n",
            "📄 File: 302.txt ∽ Similarity: 0.1033 ✒️Title: Speak easy plan for media players\n",
            "📄 File: 319.txt ∽ Similarity: 0.1021 ✒️Title: Why Cell will get the hard sell\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "⭐ Query 5: Web links and virus ⭐\n",
            "------------------------------------------------------------------------------------------\n",
            "📄 File: 210.txt ∽ Similarity: 0.3558 ✒️Title: Toxic web links help virus spread\n",
            "📄 File: 008.txt ∽ Similarity: 0.3207 ✒️Title: Virus poses as Christmas e-mail\n",
            "📄 File: 252.txt ∽ Similarity: 0.3207 ✒️Title: Virus poses as Christmas e-mail\n",
            "📄 File: 117.txt ∽ Similarity: 0.2712 ✒️Title: Joke e-mail virus tricks users\n",
            "📄 File: 281.txt ∽ Similarity: 0.1874 ✒️Title: Cyber crime booms in 2004\n",
            "📄 File: 177.txt ∽ Similarity: 0.1864 ✒️Title: Microsoft debuts security tools\n",
            "📄 File: 292.txt ∽ Similarity: 0.1864 ✒️Title: Microsoft debuts security tools\n",
            "📄 File: 385.txt ∽ Similarity: 0.1539 ✒️Title: Beckham virus spotted on the net\n",
            "📄 File: 039.txt ∽ Similarity: 0.1337 ✒️Title: Security warning over 'FBI virus'\n",
            "📄 File: 334.txt ∽ Similarity: 0.1337 ✒️Title: Security warning over 'FBI virus'\n",
            "📄 File: 272.txt ∽ Similarity: 0.1252 ✒️Title: Windows worm travels with Tetris\n",
            "📄 File: 036.txt ∽ Similarity: 0.1128 ✒️Title: Microsoft seeking spyware trojan\n",
            "📄 File: 003.txt ∽ Similarity: 0.1128 ✒️Title: Microsoft seeking spyware trojan\n",
            "📄 File: 097.txt ∽ Similarity: 0.1105 ✒️Title: Web helps collect aid donations\n",
            "📄 File: 092.txt ∽ Similarity: 0.1028 ✒️Title: Norway upholds 'Napster' ruling\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comentario:"
      ],
      "metadata": {
        "id": "8gJtY6ZG1ecE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Segunda rodada: retirando stopwords e sem \"stemming\""
      ],
      "metadata": {
        "id": "5XbHl_ql1DTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords\n"
      ],
      "metadata": {
        "id": "MpUNbXf51TOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Terceira rodada: retirando stopwords e  com \"stemming\""
      ],
      "metadata": {
        "id": "V5CphxyE1U3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conceitos"
      ],
      "metadata": {
        "id": "AmsDwmbucfYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words (BoW)\n",
        "\n",
        "The Bag of Words approach is like making a list of all the unique words you find in these books, without caring about the order or how many times each word appears. It's like counting how many times each word shows up but not worrying about the order in which they appear."
      ],
      "metadata": {
        "id": "tgZeEgiafR5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BoW = vectorizer.fit_transform(file_list)\n",
        "\n",
        "print(BoW)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDYtJhpBcgLo",
        "outputId": "76b914ea-d088-43c9-ea86-95d99e85d458"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 401)\t0.15668736734446556\n",
            "  (0, 22)\t0.9876482516132251\n",
            "  (1, 19)\t0.9876482516132251\n",
            "  (1, 401)\t0.15668736734446556\n",
            "  (2, 17)\t0.9876482516132251\n",
            "  (2, 401)\t0.15668736734446556\n",
            "  (3, 15)\t0.9876482516132251\n",
            "  (3, 401)\t0.15668736734446556\n",
            "  (4, 20)\t0.9876482516132251\n",
            "  (4, 401)\t0.15668736734446556\n",
            "  (5, 14)\t0.9876482516132251\n",
            "  (5, 401)\t0.15668736734446556\n",
            "  (6, 16)\t0.9876482516132251\n",
            "  (6, 401)\t0.15668736734446556\n",
            "  (7, 18)\t0.9876482516132251\n",
            "  (7, 401)\t0.15668736734446556\n",
            "  (8, 21)\t0.9876482516132251\n",
            "  (8, 401)\t0.15668736734446556\n",
            "  (9, 13)\t0.9876482516132251\n",
            "  (9, 401)\t0.15668736734446556\n",
            "  (10, 11)\t0.9876482516132251\n",
            "  (10, 401)\t0.15668736734446556\n",
            "  (11, 9)\t0.9876482516132251\n",
            "  (11, 401)\t0.15668736734446556\n",
            "  (12, 7)\t0.9876482516132251\n",
            "  :\t:\n",
            "  (388, 401)\t0.15668736734446556\n",
            "  (389, 371)\t0.9876482516132251\n",
            "  (389, 401)\t0.15668736734446556\n",
            "  (390, 399)\t0.9876482516132251\n",
            "  (390, 401)\t0.15668736734446556\n",
            "  (391, 394)\t0.9876482516132251\n",
            "  (391, 401)\t0.15668736734446556\n",
            "  (392, 398)\t0.9876482516132251\n",
            "  (392, 401)\t0.15668736734446556\n",
            "  (393, 395)\t0.9876482516132251\n",
            "  (393, 401)\t0.15668736734446556\n",
            "  (394, 397)\t0.9876482516132251\n",
            "  (394, 401)\t0.15668736734446556\n",
            "  (395, 396)\t0.9876482516132251\n",
            "  (395, 401)\t0.15668736734446556\n",
            "  (396, 3)\t0.9876482516132251\n",
            "  (396, 401)\t0.15668736734446556\n",
            "  (397, 2)\t0.9876482516132251\n",
            "  (397, 401)\t0.15668736734446556\n",
            "  (398, 1)\t0.9876482516132251\n",
            "  (398, 401)\t0.15668736734446556\n",
            "  (399, 400)\t0.9876482516132251\n",
            "  (399, 401)\t0.15668736734446556\n",
            "  (400, 0)\t0.9876482516132251\n",
            "  (400, 401)\t0.15668736734446556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-grams\n",
        "\n",
        "This modification will create a matrix where each column represents a bigram (a combination of two words)."
      ],
      "metadata": {
        "id": "YQzKUgjxe3KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_gramsVectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "n_grams = n_gramsVectorizer.fit_transform(file_list)\n",
        "\n",
        "print(n_grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnPnpEHRc0Lz",
        "outputId": "9755b815-66bf-49ae-9a54-a3964c86341c"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 22)\t1\n",
            "  (1, 19)\t1\n",
            "  (2, 17)\t1\n",
            "  (3, 15)\t1\n",
            "  (4, 20)\t1\n",
            "  (5, 14)\t1\n",
            "  (6, 16)\t1\n",
            "  (7, 18)\t1\n",
            "  (8, 21)\t1\n",
            "  (9, 13)\t1\n",
            "  (10, 11)\t1\n",
            "  (11, 9)\t1\n",
            "  (12, 7)\t1\n",
            "  (13, 12)\t1\n",
            "  (14, 8)\t1\n",
            "  (15, 4)\t1\n",
            "  (16, 6)\t1\n",
            "  (17, 10)\t1\n",
            "  (18, 5)\t1\n",
            "  (19, 23)\t1\n",
            "  (20, 53)\t1\n",
            "  (21, 52)\t1\n",
            "  (22, 49)\t1\n",
            "  (23, 44)\t1\n",
            "  (24, 48)\t1\n",
            "  :\t:\n",
            "  (376, 376)\t1\n",
            "  (377, 379)\t1\n",
            "  (378, 380)\t1\n",
            "  (379, 381)\t1\n",
            "  (380, 372)\t1\n",
            "  (381, 364)\t1\n",
            "  (382, 370)\t1\n",
            "  (383, 369)\t1\n",
            "  (384, 367)\t1\n",
            "  (385, 368)\t1\n",
            "  (386, 373)\t1\n",
            "  (387, 365)\t1\n",
            "  (388, 366)\t1\n",
            "  (389, 371)\t1\n",
            "  (390, 399)\t1\n",
            "  (391, 394)\t1\n",
            "  (392, 398)\t1\n",
            "  (393, 395)\t1\n",
            "  (394, 397)\t1\n",
            "  (395, 396)\t1\n",
            "  (396, 3)\t1\n",
            "  (397, 2)\t1\n",
            "  (398, 1)\t1\n",
            "  (399, 400)\t1\n",
            "  (400, 0)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HashingVectorizer\n",
        "The n_features parameter determines the number of features in the output matrix. HashingVectorizer doesn't build a vocabulary like CountVectorizer."
      ],
      "metadata": {
        "id": "RWoMcYKVe5Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "hashingV = HashingVectorizer(n_features=1000)\n",
        "\n",
        "hashing = hashingV.transform(file_list)\n",
        "\n",
        "print(hashing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfwc-RzQdkzG",
        "outputId": "4c529877-ea6f-4eb8-dfba-74ce4abc2564"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 551)\t-0.7071067811865475\n",
            "  (0, 869)\t-0.7071067811865475\n",
            "  (1, 662)\t0.7071067811865475\n",
            "  (1, 869)\t-0.7071067811865475\n",
            "  (2, 761)\t-0.7071067811865475\n",
            "  (2, 869)\t-0.7071067811865475\n",
            "  (3, 493)\t0.7071067811865475\n",
            "  (3, 869)\t-0.7071067811865475\n",
            "  (4, 716)\t-0.7071067811865475\n",
            "  (4, 869)\t-0.7071067811865475\n",
            "  (5, 832)\t-0.7071067811865475\n",
            "  (5, 869)\t-0.7071067811865475\n",
            "  (6, 402)\t-0.7071067811865475\n",
            "  (6, 869)\t-0.7071067811865475\n",
            "  (7, 15)\t-0.7071067811865475\n",
            "  (7, 869)\t-0.7071067811865475\n",
            "  (8, 241)\t0.7071067811865475\n",
            "  (8, 869)\t-0.7071067811865475\n",
            "  (9, 717)\t0.7071067811865475\n",
            "  (9, 869)\t-0.7071067811865475\n",
            "  (10, 12)\t-0.7071067811865475\n",
            "  (10, 869)\t-0.7071067811865475\n",
            "  (11, 399)\t0.7071067811865475\n",
            "  (11, 869)\t-0.7071067811865475\n",
            "  (12, 537)\t0.7071067811865475\n",
            "  :\t:\n",
            "  (388, 869)\t-0.7071067811865475\n",
            "  (389, 111)\t0.7071067811865475\n",
            "  (389, 869)\t-0.7071067811865475\n",
            "  (390, 34)\t-0.7071067811865475\n",
            "  (390, 869)\t-0.7071067811865475\n",
            "  (391, 310)\t0.7071067811865475\n",
            "  (391, 869)\t-0.7071067811865475\n",
            "  (392, 709)\t-0.7071067811865475\n",
            "  (392, 869)\t-0.7071067811865475\n",
            "  (393, 406)\t0.7071067811865475\n",
            "  (393, 869)\t-0.7071067811865475\n",
            "  (394, 686)\t-0.7071067811865475\n",
            "  (394, 869)\t-0.7071067811865475\n",
            "  (395, 149)\t-0.7071067811865475\n",
            "  (395, 869)\t-0.7071067811865475\n",
            "  (396, 283)\t0.7071067811865475\n",
            "  (396, 869)\t-0.7071067811865475\n",
            "  (397, 650)\t0.7071067811865475\n",
            "  (397, 869)\t-0.7071067811865475\n",
            "  (398, 821)\t0.7071067811865475\n",
            "  (398, 869)\t-0.7071067811865475\n",
            "  (399, 539)\t0.7071067811865475\n",
            "  (399, 869)\t-0.7071067811865475\n",
            "  (400, 16)\t-0.7071067811865475\n",
            "  (400, 869)\t-0.7071067811865475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word vs. Character N-grams\n",
        "\n",
        "You can specify whether you want word or character N-grams using the analyzer parameter. For example, to work with character bigrams (N=2) with word boundaries (like \" j\" for \"jump\"), you can do this:\n",
        "\n",
        "This will create a matrix where each column represents a character bigram with word boundaries.*italicized text*"
      ],
      "metadata": {
        "id": "5HUl2TBhfec7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams_with_boundariesV = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
        "\n",
        "bigrams = bigrams_with_boundariesV.fit_transform(file_list)\n",
        "\n",
        "print(bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u4IYd6JeA2x",
        "outputId": "89c49c4d-b3e6-4f63-a6d7-07f3904acab4"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 32)\t1\n",
            "  (0, 39)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 117)\t1\n",
            "  (0, 118)\t1\n",
            "  (0, 116)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 9)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 117)\t1\n",
            "  (1, 118)\t1\n",
            "  (1, 116)\t1\n",
            "  (1, 29)\t1\n",
            "  (1, 6)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 117)\t1\n",
            "  (2, 118)\t1\n",
            "  (2, 116)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 26)\t1\n",
            "  (2, 94)\t1\n",
            "  (3, 0)\t1\n",
            "  :\t:\n",
            "  (397, 10)\t1\n",
            "  (398, 0)\t1\n",
            "  (398, 9)\t1\n",
            "  (398, 5)\t1\n",
            "  (398, 117)\t1\n",
            "  (398, 118)\t1\n",
            "  (398, 116)\t1\n",
            "  (398, 28)\t1\n",
            "  (398, 7)\t1\n",
            "  (399, 5)\t1\n",
            "  (399, 117)\t1\n",
            "  (399, 118)\t1\n",
            "  (399, 116)\t1\n",
            "  (399, 8)\t1\n",
            "  (399, 17)\t1\n",
            "  (399, 51)\t1\n",
            "  (399, 4)\t1\n",
            "  (400, 0)\t1\n",
            "  (400, 5)\t1\n",
            "  (400, 117)\t1\n",
            "  (400, 118)\t1\n",
            "  (400, 116)\t1\n",
            "  (400, 8)\t1\n",
            "  (400, 17)\t1\n",
            "  (400, 7)\t1\n"
          ]
        }
      ]
    }
  ]
}